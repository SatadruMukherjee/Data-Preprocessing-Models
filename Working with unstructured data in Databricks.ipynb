{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30188484-b764-4402-a2fe-a4191c6a8877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Working with PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1df1672a-9ee4-41b0-a625-65be1c11bc81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install PyPDF2\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fed515-9b2b-44f5-9c8b-aacb56fe6689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "from io import BytesIO\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee33305-b241-4756-9858-a755d95a5bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format('binaryFile').load('{Put Databricks Mounted Path here}')\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa1bb0d4-3036-44ae-825d-894e08a29954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def page_number_counter(bytes_data):\n",
    "    try:\n",
    "        # Convert binary data to a file-like object\n",
    "        pdf_file = BytesIO(bytes_data)\n",
    "        # Read the PDF\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        # Return number of pages\n",
    "        return len(pdf_reader.pages)\n",
    "    except Exception as e:\n",
    "        # Handle invalid/corrupted files gracefully\n",
    "        return None\n",
    "\n",
    "# Register as Spark UDF\n",
    "page_count_udf = udf(page_number_counter, IntegerType())\n",
    "\n",
    "# Example usage\n",
    "df_with_pages = df.withColumn(\"page_count\", page_count_udf(\"content\"))\n",
    "\n",
    "df_with_pages.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e1705b-f3de-4f99-8991-48acc47676c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Working with Image Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b064803d-bd46-46d1-a3ce-61d81a72dec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"binaryFile\").load(\"{Put Databricks Mounted Path here}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec66f43-e7ed-4bd4-8a30-d806e79e3842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6817f2d-c630-47d9-8572-d34a0bc3e806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a single Python function to return both width and height\n",
    "def get_image_dimensions(bytes_data):\n",
    "    try:\n",
    "        img = Image.open(BytesIO(bytes_data))\n",
    "        return (img.width, img.height)\n",
    "    except Exception as e:\n",
    "        raise \n",
    "\n",
    "# Define return schema (two integer fields)\n",
    "\n",
    "dimensions_schema = StructType([\n",
    "\n",
    "    StructField(\"width\", IntegerType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Register UDF\n",
    "get_image_dimensions_udf = udf(get_image_dimensions, dimensions_schema)\n",
    "\n",
    "# Apply UDF and extract both columns\n",
    "df_with_dimensions = (\n",
    "    df.withColumn(\"dimensions\", get_image_dimensions_udf(\"content\"))\n",
    ")\n",
    "\n",
    "df_with_dimensions = (df_with_dimensions\n",
    "      .withColumn(\"image_width\",  df_with_dimensions[\"dimensions\"].width)\n",
    "      .withColumn(\"image_height\", df_with_dimensions[\"dimensions\"].height)\n",
    "      .drop(\"dimensions\")\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "df_with_dimensions.select(\"path\",\"image_width\",\"image_height\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8285423034433763,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Working with unstructured data in Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}