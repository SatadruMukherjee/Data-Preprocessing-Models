{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "ejltn76ulwmrzgmwevjy",
   "authorId": "1673894472736",
   "authorName": "SATADRU",
   "authorEmail": "satadru1998@gmail.com",
   "sessionId": "f59e8716-6fc2-4cda-91c4-584b1d3276eb",
   "lastEditTime": 1768963352288
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "cf8e41f5-d7c8-4f17-b034-214747c25c40",
   "metadata": {
    "language": "sql",
    "name": "cell3",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "DROP DATABASE IF EXISTS snowflake_llm_poc;\nCREATE Database snowflake_llm_poc;\nuse snowflake_llm_poc;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "804e44a3-1d64-48e2-a686-2a49c86d0e6e",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "create or replace stage snowflake_llm_poc.PUBLIC.Snow_stage_directory_table_stock_data url=\"s3://landinglayertest/mmragtest/\" \ncredentials=(aws_key_id=''\naws_secret_key='')\nDirectory=(ENABLE=TRUE);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb4e330c-2ded-4b21-b0ed-e6ce59b74415",
   "metadata": {
    "language": "sql",
    "name": "cell5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ls @snowflake_llm_poc.PUBLIC.Snow_stage_directory_table_stock_data;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2a4a71d-7e93-4ecb-8a43-93f3a22bdf5a",
   "metadata": {
    "language": "sql",
    "name": "cell11",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR Replace table snowflake_llm_poc.PUBLIC.existing_data as \n(SELECT RELATIVE_PATH, FILE_URL,\n       (AI_EMBED('voyage-multimodal-3', TO_FILE(FILE_URL))) AS image_embedding\nFROM DIRECTORY(@snowflake_llm_poc.PUBLIC.Snow_stage_directory_table_stock_data))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4176db1-d2ac-4abb-ba93-42f211557506",
   "metadata": {
    "language": "sql",
    "name": "cell12",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "select * from snowflake_llm_poc.PUBLIC.existing_data;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46200d5e-5076-4891-a15a-8d22a3265c07",
   "metadata": {
    "language": "sql",
    "name": "cell18",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "set user_question = 'What is the estimated market size of GenAI for 2032?';\nSELECT Relative_path,file_url from snowflake_llm_poc.PUBLIC.existing_data\n            ORDER BY VECTOR_L2_DISTANCE(\n            AI_EMBED('voyage-multimodal-3', \n            $user_question\n            ), image_embedding\n            ) limit 3;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d8333d6-8f48-4420-95a3-49eef6ea07aa",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "set user_question = 'What is the estimated market size of GenAI for 2032?';\n\n-- User query that we want the system to answer\n-- This question exists in the PDF and should be answered\n-- ONLY using the retrieved document images\nSET user_question = 'What is the estimated market size of GenAI for 2032?';\n\n-- Step 1: RETRIEVAL\n-- Rank images based on semantic similarity between:\n-- 1) The user question (embedded as a multimodal vector)\n-- 2) The stored image embeddings from the PDF pages\nWITH ranked_images AS (\n\n    SELECT\n        -- Relative path of the image inside the external stage\n        relative_path,\n\n        -- Compute L2 (Euclidean) distance between:\n        -- a) embedding of the user question\n        -- b) embedding of each image\n        -- Smaller distance = higher similarity\n        VECTOR_L2_DISTANCE(\n            AI_EMBED(\n                'voyage-multimodal-3',  -- Multimodal embedding model\n                $user_question          -- Natural language query\n            ),\n            image_embedding             -- Precomputed image embeddings\n        ) AS distance\n\n    FROM snowflake_llm_poc.PUBLIC.existing_data\n\n    -- Order images by semantic similarity\n    ORDER BY distance\n\n    -- Keep only the Top-K most relevant images\n    LIMIT 3\n),\n\n-- Step 2: AUGMENTATION\n-- Convert the retrieved image paths into Snowflake FILE objects\n-- These FILE objects can be passed directly to multimodal LLMs\nimage_files AS (\n\n    SELECT\n        ARRAY_AGG(\n            TO_FILE(\n                '@snowflake_llm_poc.PUBLIC.Snow_stage_directory_table_stock_data',\n                relative_path\n            )\n        ) AS files\n    FROM ranked_images\n)\n\n-- Step 3: GENERATION\n-- Use a multimodal LLM to generate an answer\n-- The model is explicitly instructed to rely ONLY on the retrieved images\nSELECT AI_COMPLETE(\n    'pixtral-large',\n    PROMPT(\n        'INSTRUCTIONS:\n        Answer the QUERY using ONLY the CONTEXT provided below.\n        Keep the answer strictly grounded in the given context.\n        If the context does not contain enough information to answer the query,\n        respond with: \"I do not have enough context to respond to this query.\"\n\n        CONTEXT:\n        Document Image 1: {0}\n        Document Image 2: {1}\n        Document Image 3: {2}\n\n        QUERY:\n        {3}',\n\n        -- Top-K retrieved images from similarity search\n        files[0],\n        files[1],\n        files[2],\n\n        -- Original user question\n        $user_question\n    )\n) AS answer\nFROM image_files;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e511e90-74a6-4b72-a287-4a651763125b",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": "-- User query that we want the system to answer\n-- This question exists in the PDF and should be answered\n-- ONLY using the retrieved document images\nset user_question = 'Why customers choose AWS?';\n\n-- Step 1: RETRIEVAL\n-- Rank images based on semantic similarity between:\n-- 1) The user question (embedded as a multimodal vector)\n-- 2) The stored image embeddings from the PDF pages\nWITH ranked_images AS (\n\n    SELECT\n        -- Relative path of the image inside the external stage\n        relative_path,\n\n        -- Compute L2 (Euclidean) distance between:\n        -- a) embedding of the user question\n        -- b) embedding of each image\n        -- Smaller distance = higher similarity\n        VECTOR_L2_DISTANCE(\n            AI_EMBED(\n                'voyage-multimodal-3',  -- Multimodal embedding model\n                $user_question          -- Natural language query\n            ),\n            image_embedding             -- Precomputed image embeddings\n        ) AS distance\n\n    FROM snowflake_llm_poc.PUBLIC.existing_data\n\n    -- Order images by semantic similarity\n    ORDER BY distance\n\n    -- Keep only the Top-K most relevant images\n    LIMIT 3\n),\n\n-- Step 2: AUGMENTATION\n-- Convert the retrieved image paths into Snowflake FILE objects\n-- These FILE objects can be passed directly to multimodal LLMs\nimage_files AS (\n\n    SELECT\n        ARRAY_AGG(\n            TO_FILE(\n                '@snowflake_llm_poc.PUBLIC.Snow_stage_directory_table_stock_data',\n                relative_path\n            )\n        ) AS files\n    FROM ranked_images\n)\n\n-- Step 3: GENERATION\n-- Use a multimodal LLM to generate an answer\n-- The model is explicitly instructed to rely ONLY on the retrieved images\nSELECT AI_COMPLETE(\n    'pixtral-large',\n    PROMPT(\n        'INSTRUCTIONS:\n        Answer the QUERY using ONLY the CONTEXT provided below.\n        Keep the answer strictly grounded in the given context.\n        If the context does not contain enough information to answer the query,\n        respond with: \"I do not have enough context to respond to this query.\"\n\n        CONTEXT:\n        Document Image 1: {0}\n        Document Image 2: {1}\n        Document Image 3: {2}\n\n        QUERY:\n        {3}',\n\n        -- Top-K retrieved images from similarity search\n        files[0],\n        files[1],\n        files[2],\n\n        -- Original user question\n        $user_question\n    )\n) AS answer\nFROM image_files;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12f25f94-6c1e-4853-ac30-5b87770fab0b",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "\" Customers choose AWS for several reasons:\\n\\n1. **Experience**: AWS has 18 years of experience helping millions of customers.\\n2. **Global Reach**: AWS spans 36 regions and 114 Availability Zones.\\n3. **Security**: AWS offers over 300 security features.\\n4. **Innovation**: AWS provides over 250 service offerings.\\n5. **AWS Infrastructure**: AWS infrastructure is 3.6 times more energy efficient than the median of surveyed U.S. enterprise data centers.\\n6. **Total Cost of Ownership (TCO)**: AWS has achieved 151 price reductions since 2006.\\n7. **Ecosystem**: AWS ecosystem includes 12,000 software listings from 2,000 Independent Software Vendors (ISVs).\""
  },
  {
   "cell_type": "code",
   "id": "a177a517-9a1f-4590-999d-832417ef25e5",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "-- User query that we want the system to answer\n-- This question exists in the PDF and should be answered\n-- ONLY using the retrieved document images\nset user_question = 'How did Standard Chartered reduce its risk grid compute costs by using AWS?';\n\n-- Step 1: RETRIEVAL\n-- Rank images based on semantic similarity between:\n-- 1) The user question (embedded as a multimodal vector)\n-- 2) The stored image embeddings from the PDF pages\nWITH ranked_images AS (\n\n    SELECT\n        -- Relative path of the image inside the external stage\n        relative_path,\n\n        -- Compute L2 (Euclidean) distance between:\n        -- a) embedding of the user question\n        -- b) embedding of each image\n        -- Smaller distance = higher similarity\n        VECTOR_L2_DISTANCE(\n            AI_EMBED(\n                'voyage-multimodal-3',  -- Multimodal embedding model\n                $user_question          -- Natural language query\n            ),\n            image_embedding             -- Precomputed image embeddings\n        ) AS distance\n\n    FROM snowflake_llm_poc.PUBLIC.existing_data\n\n    -- Order images by semantic similarity\n    ORDER BY distance\n\n    -- Keep only the Top-K most relevant images\n    LIMIT 3\n),\n\n-- Step 2: AUGMENTATION\n-- Convert the retrieved image paths into Snowflake FILE objects\n-- These FILE objects can be passed directly to multimodal LLMs\nimage_files AS (\n\n    SELECT\n        ARRAY_AGG(\n            TO_FILE(\n                '@snowflake_llm_poc.PUBLIC.Snow_stage_directory_table_stock_data',\n                relative_path\n            )\n        ) AS files\n    FROM ranked_images\n)\n\n-- Step 3: GENERATION\n-- Use a multimodal LLM to generate an answer\n-- The model is explicitly instructed to rely ONLY on the retrieved images\nSELECT AI_COMPLETE(\n    'pixtral-large',\n    PROMPT(\n        'INSTRUCTIONS:\n        Answer the QUERY using ONLY the CONTEXT provided below.\n        Keep the answer strictly grounded in the given context.\n        If the context does not contain enough information to answer the query,\n        respond with: \"I do not have enough context to respond to this query.\"\n\n        CONTEXT:\n        Document Image 1: {0}\n        Document Image 2: {1}\n        Document Image 3: {2}\n\n        QUERY:\n        {3}',\n\n        -- Top-K retrieved images from similarity search\n        files[0],\n        files[1],\n        files[2],\n\n        -- Original user question\n        $user_question\n    )\n) AS answer\nFROM image_files;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "67c6a77b-c2c6-466c-87f2-86e775ab3a4b",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "\" Standard Chartered reduced its risk grid compute costs by moving to AWS, specifically by utilizing Amazon EC2 Spot Instances. This migration allowed the bank to triple its compute capacity and reduce its compute costs by 60%.\""
  }
 ]
}