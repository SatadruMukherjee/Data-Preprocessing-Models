Code:
=======

Task1:
=======
print("Test Job Parameter")
table_name=dbutils.widgets.get("table_name")
print("The input table name : ",table_name)
query = f"SELECT COUNT(1) as count FROM {table_name}"
df = spark.sql(query)
row_count = df.collect()[0]["count"]  # Store the count value in a variable
#print(f"Row count stored in variable: {row_count}")
dbutils.jobs.taskValues.set(key = "record_count", value = row_count)

Task2:
=======
count_value = dbutils.jobs.taskValues.get(taskKey = "task_1", key = "record_count")
print("Count of rows in the table : ",count_value)


Trigger Databricks jobs from Rest API:
========================================
import requests
import json

# Set your Databricks workspace URL and the personal access token
DATABRICKS_URL = ''  # Replace with your Databricks URL
TOKEN = ''  # Replace with your Databricks personal access token


# Set up the API endpoint
endpoint = f'{DATABRICKS_URL}/api/2.2/jobs/run-now'

# Prepare the payload with the new branch or tag
payload = {
  "job_id": {Workflow ID},
  "job_parameters": {
    "key": "value"  }
}

# Define the headers including the authorization token
headers = {
    'Authorization': f'Bearer {TOKEN}',
    'Content-Type': 'application/json'
}

# Send the PATCH request to update the repo
response = requests.post(endpoint, headers=headers, data=json.dumps(payload))

# Check if the request was successful
if response.status_code == 200:
    # If successful, print the updated repo details
    repo_details = response.json()
    print(repo_details)
else:
    # If the request failed, print the error message
    print(f"Failed to update repo: {response.status_code} - {response.text}")
	
Read excel data using Spark:
===============================
#sample data file path
sampleDataFilePath = "/mnt/excel_test_external_stage/dummydata.xlsx"

#read excelfile
sample1DF = spark.read.format("com.crealytics.spark.excel") \
.option("header", True) \
.option("inferSchema", True) \
.load(sampleDataFilePath)

print("Data read from excel is complete..")

display(sample1DF)

	
Create Databricks workflow using Rest API:
===========================================
import requests
import json

# Set your Databricks workspace URL and the personal access token
DATABRICKS_URL = ''  # Replace with your Databricks URL
TOKEN = ''  # Replace with your Databricks personal access token


# Set up the API endpoint
endpoint = f'{DATABRICKS_URL}/api/2.2/jobs/create'

# Prepare the payload with the new branch or tag
payload = {
    "name": "Job creation using Rest API demo YT",
    "email_notifications": {
      "no_alert_for_skipped_runs": False
    },
    "webhook_notifications": {},
    "notification_settings": {
      "no_alert_for_skipped_runs": False,
      "no_alert_for_canceled_runs": False
    },
    "timeout_seconds": 0,
    "max_concurrent_runs": 1,
    "tasks": [
      {
        "task_key": "task_1",
        "run_if": "ALL_SUCCESS",
        "notebook_task": {
          "notebook_path": "/Workspace/Users/{Your user name}/count_computer",
          "source": "WORKSPACE"
        },
        "job_cluster_key": "transient_cluster_demo_yt",
        "libraries": [
          {
            "pypi": {
              "package": "pandarallel==1.6.5"
            }
          },
          {
            "pypi": {
              "package": "github-action-utils==1.1.0"
            }
          },
          {
            "maven": {
              "coordinates": "com.crealytics:spark-excel_2.12:3.5.1_0.20.4"
            }
          }
        ],
        "timeout_seconds": 0,
        "email_notifications": {},
        "webhook_notifications": {}
      },
      {
        "task_key": "Run_task_2_only_if_count_is_greater_than_5",
        "depends_on": [
          {
            "task_key": "task_1"
          }
        ],
        "run_if": "ALL_SUCCESS",
        "condition_task": {
          "op": "GREATER_THAN",
          "left": "{{tasks.task_1.values.record_count}}",
          "right": "6"
        },
        "timeout_seconds": 0,
        "email_notifications": {},
        "webhook_notifications": {}
      },
      {
        "task_key": "task_2",
        "depends_on": [
          {
            "task_key": "Run_task_2_only_if_count_is_greater_than_5",
            "outcome": "true"
          }
        ],
        "run_if": "ALL_SUCCESS",
        "notebook_task": {
          "notebook_path": "/Workspace/Users/{Your user name}/count_printer",
          "source": "WORKSPACE"
        },
        "job_cluster_key": "transient_cluster_demo_yt",
        "timeout_seconds": 0,
        "email_notifications": {},
        "webhook_notifications": {}
      },
      {
        "task_key": "excel_read",
        "depends_on": [
          {
            "task_key": "task_2"
          }
        ],
        "run_if": "ALL_SUCCESS",
        "notebook_task": {
          "notebook_path": "/Workspace/Users/{Your user name}/read_excel_maven_lib_demo",
          "source": "WORKSPACE"
        },
        "job_cluster_key": "transient_cluster_demo_yt",
        "timeout_seconds": 0,
        "email_notifications": {},
        "webhook_notifications": {}
      }
    ],
    "job_clusters": [
      {
        "job_cluster_key": "transient_cluster_demo_yt",
        "new_cluster": {
          "cluster_name": "",
          "spark_version": "15.4.x-scala2.12",
          "spark_conf": {
            "spark.master": "local[*, 4]",
            "spark.databricks.cluster.profile": "singleNode"
          },
          "aws_attributes": {
            "first_on_demand": 1,
            "availability": "SPOT_WITH_FALLBACK",
            "zone_id": "us-east-1a",
            "spot_bid_price_percent": 100
          },
          "node_type_id": "i3.xlarge",
          "driver_node_type_id": "i3.xlarge",
          "custom_tags": {
            "ResourceClass": "SingleNode"
          },
          "spark_env_vars": {
            "PYSPARK_PYTHON": "/databricks/python3/bin/python3"
          },
          "enable_elastic_disk": False,
          "data_security_mode": "SINGLE_USER",
          "runtime_engine": "STANDARD",
          "num_workers": 0
        }
      }
    ],
    "format": "MULTI_TASK",
    "queue": {
      "enabled": True
    },
    "parameters": [
      {
        "name": "table_name",
        "default": "Put the table name"
      }
    ]
  }

# Define the headers including the authorization token
headers = {
    'Authorization': f'Bearer {TOKEN}',
    'Content-Type': 'application/json'
}

# Send the PATCH request to update the repo
response = requests.post(endpoint, headers=headers, data=json.dumps(payload))

# Check if the request was successful
if response.status_code == 200:
    # If successful, print the updated repo details
    workflow_details = response.json()
    print(workflow_details)
else:
    # If the request failed, print the error message
    print(f"Failed to create workflow: {response.status_code} - {response.text}")
