{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18e4d56-775b-4fbb-b40e-d4ca94307f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain databricks-langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9c2f23-6f45-4027-ae71-343a89b8a6be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e93c1276-2f28-4370-a813-323b37630a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281c1ad4-fd29-4149-b698-feaf8b0e5143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"DATABRICKS_HOST\"] = \"{Put your Databricks Host(URL) here}\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f75689-af1c-4fa7-8e04-6a201260ef6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatDatabricks(endpoint=\"databricks-llama-4-maverick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb08480a-8e49-4b13-a0a3-bf3989259312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Asking generic questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd28a4b-77c6-4f93-84c4-e1f18cc2d42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AIMessage(content='The Prime Minister of India from 2014 is Narendra Modi. He has been serving as the Prime Minister of India since May 26, 2014, and was re-elected for a second term in 2019.', additional_kwargs={}, response_metadata={'usage': {'prompt_tokens': 23, 'completion_tokens': 48, 'total_tokens': 71}, 'prompt_tokens': 23, 'completion_tokens': 48, 'total_tokens': 71, 'model': 'meta-llama-4-maverick-040225', 'model_name': 'meta-llama-4-maverick-040225', 'finish_reason': 'stop'}, id='lc_run--bc42bd26-6e00-455e-94c1-822d27cec8e2-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Who is the Prime Minister of Inida from 2014?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfdd335-4d4b-493d-b3b8-43cd763596cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Asking questions which require real-time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ee80766-885a-45fe-a499-2398564be1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m not capable of accessing real-time information, so I can\\'t provide you with the current time. However, I can suggest a few alternatives to find out the current time:\\n\\n1. Check your device: You can check the time on your phone, watch, or computer.\\n2. Use a search engine: You can type \"current time\" in a search engine like Google, and it will display the current time based on your location or a specific time zone if you specify it.\\n3. Look at a clock: If you\\'re near a clock, you can simply look at it to know the current time.\\n\\nIf you need to know the time in a specific time zone or city, feel free to let me know, and I can try to help you with that!', additional_kwargs={}, response_metadata={'usage': {'prompt_tokens': 16, 'completion_tokens': 154, 'total_tokens': 170}, 'prompt_tokens': 16, 'completion_tokens': 154, 'total_tokens': 170, 'model': 'meta-llama-4-maverick-040225', 'model_name': 'meta-llama-4-maverick-040225', 'finish_reason': 'stop'}, id='lc_run--4d86edde-c867-45d7-b4a4-1350207ccf3c-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is the current time?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6304f327-45f6-42b1-89df-b83cefbbff3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tool creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad63e141-8299-4b8a-8276-7e02292adbaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "class ToolsList:\n",
    "    @tool\n",
    "    def get_current_time() -> str:\n",
    "        \"\"\"\n",
    "        Returns the current timestamp.\n",
    "\n",
    "        This tool fetches the system's current local date and time. \n",
    "        It is useful for tasks where the LLM needs to reference the actual \n",
    "        current time instead of reasoning about it contextually.\n",
    "\n",
    "        Returns:\n",
    "            str: The current local timestamp in the format YYYY-MM-DD HH:MM:SS.\n",
    "        \"\"\"\n",
    "        from datetime import datetime\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "    @tool\n",
    "    def custom_calculator(expression: str) -> str:\n",
    "        \"\"\"\n",
    "        Evaluates a mathematical expression provided as a string.\n",
    "\n",
    "        This tool safely parses and evaluates arithmetic expressions such as \n",
    "        addition, subtraction, multiplication, division, and parentheses-based \n",
    "        grouping. It filters out invalid characters before evaluation to prevent \n",
    "        code injection or unsafe execution.\n",
    "\n",
    "        Args:\n",
    "            expression (str): A mathematical expression, e.g., \"2 + 3 * (5 - 1)\".\n",
    "\n",
    "        Returns:\n",
    "            str: The result of the evaluated expression as a string.\n",
    "                Returns an error message if the expression is invalid or \n",
    "                leads to a runtime error (e.g., division by zero).\n",
    "\n",
    "        Example:\n",
    "            >>> custom_calculator(\"10 / 2 + 3\")\n",
    "            '8.0'\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        # Remove any non-digit or non-operator characters\n",
    "        expression = re.sub(r'[^0-9+\\-*/().]', '', expression)\n",
    "        try:\n",
    "            result = eval(expression)\n",
    "            return str(result)\n",
    "        except (SyntaxError, ZeroDivisionError, NameError, TypeError, OverflowError):\n",
    "            return \"Error: Invalid expression\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29bde538-e561-4269-aadd-6345d2b47a77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tool binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe73917-e766-435d-bf9b-1da3aa0d9e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tools_list = [ToolsList.get_current_time, ToolsList.custom_calculator]\n",
    "llm_with_tools = llm.bind_tools(tools_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9ce5d5-0791-469d-a35c-3e80b8a5f09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46155ef1-8373-4197-9a49-db87d764f8fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'usage': {'prompt_tokens': 693, 'completion_tokens': 10, 'total_tokens': 703}, 'prompt_tokens': 693, 'completion_tokens': 10, 'total_tokens': 703, 'model': 'meta-llama-4-maverick-040225', 'model_name': 'meta-llama-4-maverick-040225', 'finish_reason': 'stop'}, id='lc_run--fadf55aa-7289-4509-8368-cfdbd3d1994a-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg = llm_with_tools.invoke(\"Hello world!\")\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2253b125-9bf6-4790-ae1b-d0049170fcb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7eb551ba-48ab-416f-9542-3f0bf826c225', 'function': {'arguments': '{}', 'name': 'get_current_time'}, 'type': 'function'}]}, response_metadata={'usage': {'prompt_tokens': 696, 'completion_tokens': 5, 'total_tokens': 701}, 'prompt_tokens': 696, 'completion_tokens': 5, 'total_tokens': 701, 'model': 'meta-llama-4-maverick-040225', 'model_name': 'meta-llama-4-maverick-040225', 'finish_reason': 'tool_calls'}, id='lc_run--3a961c0c-bb8f-4b0a-852f-80ccbe344b9e-0', tool_calls=[{'name': 'get_current_time', 'args': {}, 'id': 'call_7eb551ba-48ab-416f-9542-3f0bf826c225', 'type': 'tool_call'}])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg = llm_with_tools.invoke(\"What is the Current Time?\")\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ecdfc1-366c-426f-afe6-d269632bd2d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tool execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f66071b-a332-4981-bfbd-9f9cde94d502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='2025-10-23 17:13:06' name='get_current_time' tool_call_id='call_7eb551ba-48ab-416f-9542-3f0bf826c225'\n"
     ]
    }
   ],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"get_current_time\": ToolsList.get_current_time, \"custom_calculator\": ToolsList.custom_calculator}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    print(tool_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c8e463-b46f-4df8-a418-82e30a2ba9b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Beautification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6d43e5-15f8-4afd-a608-fcf1847f4c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter your question here:  What is the result of 1,984,135 * 9,343,116?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Input for output generation: \n[HumanMessage(content='What is the result of 1,984,135 * 9,343,116?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_c4d2e30e-57e7-41b7-97a7-1b1bb4f5b1a1', 'function': {'arguments': '{\"expression\": \"1984135 * 9343116\"}', 'name': 'custom_calculator'}, 'type': 'function'}]}, response_metadata={'usage': {'prompt_tokens': 709, 'completion_tokens': 16, 'total_tokens': 725}, 'prompt_tokens': 709, 'completion_tokens': 16, 'total_tokens': 725, 'model': 'meta-llama-4-maverick-040225', 'model_name': 'meta-llama-4-maverick-040225', 'finish_reason': 'tool_calls'}, id='lc_run--200228d6-4048-4e9e-8eb8-66b3b2309080-0', tool_calls=[{'name': 'custom_calculator', 'args': {'expression': '1984135 * 9343116'}, 'id': 'call_c4d2e30e-57e7-41b7-97a7-1b1bb4f5b1a1', 'type': 'tool_call'}]), ToolMessage(content='18538003464660', name='custom_calculator', tool_call_id='call_c4d2e30e-57e7-41b7-97a7-1b1bb4f5b1a1')]\ncontent='The result of 1,984,135 * 9,343,116 is 18,538,003,464,660.' additional_kwargs={} response_metadata={'usage': {'prompt_tokens': 741, 'completion_tokens': 29, 'total_tokens': 770}, 'prompt_tokens': 741, 'completion_tokens': 29, 'total_tokens': 770, 'model': 'meta-llama-4-maverick-040225', 'model_name': 'meta-llama-4-maverick-040225', 'finish_reason': 'stop'} id='lc_run--0d621655-75aa-4e58-ab36-ed8488629795-0'\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Take a question from the user\n",
    "# This simulates a real chat scenario where the user asks something\n",
    "user_query = input(\"Enter your question here: \")\n",
    "\n",
    "# Step 2: Import HumanMessage from LangChain core messages\n",
    "# HumanMessage is used to wrap the user's input in a message object\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Step 3: Create a list of messages to maintain conversation history\n",
    "# Start with the user's question wrapped as a HumanMessage\n",
    "messages = [HumanMessage(user_query)]\n",
    "\n",
    "# Step 4: Invoke the LLM with tools using the current conversation messages\n",
    "# llm_with_tools knows about the bound tools (get_current_time, custom_calculator)\n",
    "# The model may decide to call a tool based on the user query\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "# Step 5: Check if the model wants to call any tools\n",
    "# If tool_calls is not empty, the model has decided that a tool invocation is required\n",
    "if ai_msg.tool_calls!=[]:\n",
    "    messages.append(ai_msg)\n",
    "\n",
    "    # Step 6: Iterate through any tool calls the model has requested\n",
    "    # ai_msg.tool_calls contains information about which tool(s) to invoke\n",
    "    # Each tool_call has a 'name' and 'args' dictionary\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        \n",
    "        # Step 6a: Map the tool name from the tool_call to the actual Python function\n",
    "        # We lowercase the tool name to ensure matching is case-insensitive\n",
    "        selected_tool = {\n",
    "            \"get_current_time\": ToolsList.get_current_time,\n",
    "            \"custom_calculator\": ToolsList.custom_calculator\n",
    "        }[tool_call[\"name\"].lower()]\n",
    "\n",
    "        # Step 6b: Invoke the selected tool with the arguments provided by the model\n",
    "        # The tool returns a ToolMessage containing the output of the function\n",
    "        tool_msg = selected_tool.invoke(tool_call)\n",
    "\n",
    "        # Step 6c: Append the tool's output to the messages list\n",
    "        # This keeps the conversation history complete: HumanMessage -> AIMessage -> ToolMessage\n",
    "        messages.append(tool_msg)\n",
    "\n",
    "    # Step 7: Pass the updated conversation (user query + AI response + tool outputs) back to the LLM\n",
    "    # This allows the model to generate a final \"beautified\" response incorporating:\n",
    "    #   1. The original user question (HumanMessage)\n",
    "    #   2. Its intermediate reasoning or AI response (AIMessage)\n",
    "    #   3. Any executed tool outputs (ToolMessage)\n",
    "    # The final response will be coherent and naturally formatted for the user\n",
    "    print(\"Final Input for output generation: \")\n",
    "    print(messages)\n",
    "    final_response = llm_with_tools.invoke(messages)\n",
    "\n",
    "    # Step 8: Optionally, print the final response to see the beautified output\n",
    "    print(final_response)\n",
    "\n",
    "# Case when no tool is required\n",
    "# If the model did not request any tool, the AIMessage content is already the final answer\n",
    "else:\n",
    "    print(ai_msg.content)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM Function Calling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}