{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTxLwa2zA89nduzR/lP88X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Clear the current Catalog Path if exists**"
      ],
      "metadata": {
        "id": "-7P5GATSEiIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/spark-warehouse"
      ],
      "metadata": {
        "id": "YuQhlbGHFrNY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install required modules**"
      ],
      "metadata": {
        "id": "W5ufjdrLEnss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install delta-spark==2.2.0\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuoPaCqEpeAl",
        "outputId": "4af7b120-497a-43d1-9b60-b187e59f8efd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting delta-spark==2.2.0\n",
            "  Downloading delta_spark-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pyspark<3.4.0,>=3.3.0 (from delta-spark==2.2.0)\n",
            "  Downloading pyspark-3.3.4.tar.gz (281.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from delta-spark==2.2.0) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.0.0->delta-spark==2.2.0) (3.23.0)\n",
            "Collecting py4j==0.10.9.5 (from pyspark<3.4.0,>=3.3.0->delta-spark==2.2.0)\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading delta_spark-2.2.0-py3-none-any.whl (20 kB)\n",
            "Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.4-py2.py3-none-any.whl size=281945742 sha256=6cfe7067b81b31bb66e18c596dbbbaaac6bec64d35c7f241e8b19a767b270b4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/72/4b/16f4dac805dc367788b299db21fc03b3a26d396d883d29c454\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark, delta-spark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed delta-spark-2.2.0 py4j-0.10.9.5 pyspark-3.3.4\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.3.4)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup Spark & DeltaLake**"
      ],
      "metadata": {
        "id": "PV7rKJqgEqxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from delta import *\n",
        "\n",
        "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
      ],
      "metadata": {
        "id": "cGfAa-dTpeWd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "98bcMlPipfi1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create the Database & tables**"
      ],
      "metadata": {
        "id": "Kh7k2h3LExCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = \"demo_delta_cdf\"\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObLfDXrbpmnB",
        "outputId": "e162260c-669d-4125-c67c-5190d833e9b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_table = f\"{db}.product_sales_src\"\n",
        "target_table = f\"{db}.product_sales_tgt\""
      ],
      "metadata": {
        "id": "fzKF13Cqr_pC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"DROP TABLE IF EXISTS {source_table}\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5l_krKasq1-",
        "outputId": "1328cb31-2683-415a-a04d-2e6a0d85c442"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"\n",
        "CREATE or replace TABLE {source_table} (\n",
        "  product_id   STRING,\n",
        "  sales_date   DATE,\n",
        "  quantity     INT,\n",
        "  revenue      DOUBLE\n",
        ") using Delta TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
        "-- CDF is enabled on this table so that any inserts, updates, or deletes can be captured\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJpX0lN5ssTG",
        "outputId": "aa79758e-eda8-478b-a39c-671a21494cf9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"\n",
        "INSERT INTO {source_table} VALUES\n",
        "('P001','2025-08-01',10,100.0),\n",
        "('P001','2025-08-02',20,200.0),\n",
        "('P002','2025-08-01',5,  50.0),\n",
        "('P003','2025-08-01',7,  70.0)\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L8k6EcSsxuH",
        "outputId": "0517d180-148f-4af6-c83c-39f8746c064c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"\n",
        "CREATE or replace TABLE {target_table} (\n",
        "  product_id   STRING,\n",
        "  sales_date   DATE,\n",
        "  quantity     INT,\n",
        "  revenue      DOUBLE\n",
        ") using Delta\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp7x-lbZstun",
        "outputId": "7e0afc1e-b6f2-4058-bb5d-7146c605bfa2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"select * from {source_table}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpIe-lmzhdDY",
        "outputId": "f8892e48-b14b-4b9b-8a74-04b2c68edb24"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P002      |2025-08-01|5       |50.0   |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inspect the changed data post inster**"
      ],
      "metadata": {
        "id": "qBZofngjE3bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cdc_df = spark.read.format(\"delta\") \\\n",
        "    .option(\"readChangeFeed\", \"true\") \\\n",
        "    .option(\"startingTimestamp\", \"1970-01-01 00:00:00\") \\\n",
        "     .table(f\"{source_table}\")\n",
        "cdc_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6WFfoZJuCPD",
        "outputId": "b12dc600-3edd-4a7c-aa02-61faec03641b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type|_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "|P001      |2025-08-01|10      |100.0  |insert      |1              |2025-08-18 16:28:09.733|\n",
            "|P001      |2025-08-02|20      |200.0  |insert      |1              |2025-08-18 16:28:09.733|\n",
            "|P002      |2025-08-01|5       |50.0   |insert      |1              |2025-08-18 16:28:09.733|\n",
            "|P003      |2025-08-01|7       |70.0   |insert      |1              |2025-08-18 16:28:09.733|\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We will be creating SQL merge statement dynamically, but this cdc df is not available as table, for that we need to create a temp view which can act as source table for us**"
      ],
      "metadata": {
        "id": "-dIADyjSbI4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_temp_view = f\"{source_table}_view\".replace(\".\", \"_\")\n",
        "print(\"The temp view which will act as source for our Merge Statement : \",source_temp_view)\n",
        "cdc_df.createOrReplaceTempView(source_temp_view)\n",
        "spark.sql(f\"select * from {source_temp_view}\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL49pDph3Uit",
        "outputId": "1a7a79cd-a187-411e-8733-80de40462444"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The temp view which will act as source for our Merge Statement :  demo_delta_cdf_product_sales_src_view\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type|_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "|P001      |2025-08-01|10      |100.0  |insert      |1              |2025-08-18 16:28:09.733|\n",
            "|P001      |2025-08-02|20      |200.0  |insert      |1              |2025-08-18 16:28:09.733|\n",
            "|P002      |2025-08-01|5       |50.0   |insert      |1              |2025-08-18 16:28:09.733|\n",
            "|P003      |2025-08-01|7       |70.0   |insert      |1              |2025-08-18 16:28:09.733|\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keep the data columns & primary key columns stored in variables**"
      ],
      "metadata": {
        "id": "SGPICBoqFIpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_cols = {\"_change_type\", \"_commit_version\", \"_commit_timestamp\"}\n",
        "data_columns = [c for c in cdc_df.columns if c not in meta_cols]\n",
        "print(f\"The Data Columns in this case: {data_columns}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7Dp5DQF2wXR",
        "outputId": "d1154e9c-2777-4ff7-df7b-7e469fce8515"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Data Columns in this case: ['product_id', 'sales_date', 'quantity', 'revenue']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "primary_key_columns = ['product_id', 'sales_date']"
      ],
      "metadata": {
        "id": "Qo036cxq3Fq0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Insert section of Merge Statement**"
      ],
      "metadata": {
        "id": "heXMOZjdFQOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_merge_statement_generator(source_temp_view_name, target_delta_table_name, data_columns, primary_key_columns):\n",
        "    # Initialize an empty string to build the merge SQL statement\n",
        "    merge_statement = \"\"\n",
        "\n",
        "    # Begin the MERGE INTO statement with target table and source view\n",
        "    merge_statement += f\"\"\"MERGE INTO {target_delta_table_name} target USING {source_temp_view_name} source ON \"\"\"\n",
        "\n",
        "    # Create a list to hold join expressions between primary key columns of source and target\n",
        "    join_expression_list = []\n",
        "\n",
        "    # For each primary key column, build the equality condition (target.pk = source.pk)\n",
        "    for pk in primary_key_columns:\n",
        "        join_expression_list.append(f\"target.{pk} = source.{pk}\")\n",
        "\n",
        "    # Join all primary key conditions with \"AND\" to form the final join expression\n",
        "    join_expression = \" AND \".join(join_expression_list)\n",
        "\n",
        "    # Append the join expression to the merge statement\n",
        "    merge_statement += join_expression\n",
        "\n",
        "    # Convert the list of data columns into a comma-separated string for the INSERT part\n",
        "    data_columns_str = \", \".join(data_columns)\n",
        "\n",
        "    # Add the NOT MATCHED clause for insert operations, with column list\n",
        "    merge_statement += f\"\"\" WHEN NOT MATCHED THEN INSERT ({data_columns_str}) Values (\"\"\"\n",
        "\n",
        "    # Map each column in data_columns to its \"source.column_name\" counterpart\n",
        "    merge_statement += \", \".join([f\"source.{c}\" for c in data_columns])\n",
        "\n",
        "    # Close the VALUES parenthesis\n",
        "    merge_statement += \")\"\n",
        "\n",
        "    # Return the dynamically built merge statement\n",
        "    return merge_statement\n"
      ],
      "metadata": {
        "id": "YyfKHIglxka2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_statement =  dynamic_merge_statement_generator(source_temp_view, target_table, data_columns, primary_key_columns)\n",
        "print(merge_statement)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytKUk_fbo5Wb",
        "outputId": "7082f453-4b1b-4e1c-83df-f327c18a05fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MERGE INTO demo_delta_cdf.product_sales_tgt target USING demo_delta_cdf_product_sales_src_view source ON target.product_id = source.product_id AND target.sales_date = source.sales_date WHEN NOT MATCHED THEN INSERT (product_id, sales_date, quantity, revenue) Values (source.product_id, source.sales_date, source.quantity, source.revenue)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Target Table before First Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)\n",
        "spark.sql(merge_statement)\n",
        "print(\"Target Table after First Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kimqqAWDA_fB",
        "outputId": "90995f32-a361-4a42-b539-edd80149bf27"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Table before First Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table after First Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P002      |2025-08-01|5       |50.0   |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Delete Case in Source Table**"
      ],
      "metadata": {
        "id": "g6dVxIVGFbpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "current_timestamp = spark.sql(\"select current_timestamp\").collect()[0][0]\n",
        "# DELETE sales record (product completely withdrawn that day)\n",
        "spark.sql(f\"DELETE FROM {source_table} WHERE product_id='P002' AND sales_date='2025-08-01'\")\n",
        "cdc_df = spark.read.format(\"delta\") \\\n",
        "    .option(\"readChangeFeed\", \"true\") \\\n",
        "    .option(\"startingTimestamp\", current_timestamp) \\\n",
        "     .table(f\"{source_table}\")\n",
        "cdc_df.createOrReplaceTempView(source_temp_view)\n",
        "spark.sql(f\"select * from {source_temp_view}\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agrEaLLU55CT",
        "outputId": "159f1f8c-8ef3-4526-df4e-7cb46a89c484"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type|_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "|P002      |2025-08-01|5       |50.0   |delete      |2              |2025-08-18 16:28:57.969|\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_merge_statement_generator(source_temp_view_name, target_delta_table_name, data_columns, primary_key_columns):\n",
        "    # Initialize an empty string to build the merge SQL statement\n",
        "    merge_statement = \"\"\n",
        "\n",
        "    # Begin the MERGE INTO statement with target table and source view\n",
        "    merge_statement += f\"\"\"MERGE INTO {target_delta_table_name} target USING {source_temp_view_name} source ON \"\"\"\n",
        "\n",
        "    # Create a list to hold join expressions between primary key columns of source and target\n",
        "    join_expression_list = []\n",
        "\n",
        "    # For each primary key column, build the equality condition (target.pk = source.pk)\n",
        "    for pk in primary_key_columns:\n",
        "        join_expression_list.append(f\"target.{pk} = source.{pk}\")\n",
        "\n",
        "    # Join all primary key conditions with \"AND\" to form the final join expression\n",
        "    join_expression = \" AND \".join(join_expression_list)\n",
        "\n",
        "    # Append the join expression to the merge statement\n",
        "    merge_statement += join_expression\n",
        "\n",
        "    ## Add delete clause for matched rows with change_type = delete\n",
        "    merge_statement += f\" WHEN MATCHED AND source._change_type = 'delete' THEN DELETE \"\n",
        "\n",
        "    # Convert the list of data columns into a comma-separated string for the INSERT part\n",
        "    data_columns_str = \", \".join(data_columns)\n",
        "\n",
        "    # Add the NOT MATCHED clause for insert operations, with column list\n",
        "    merge_statement += f\"\"\" WHEN NOT MATCHED THEN INSERT ({data_columns_str}) Values (\"\"\"\n",
        "\n",
        "    # Map each column in data_columns to its \"source.column_name\" counterpart\n",
        "    merge_statement += \", \".join([f\"source.{c}\" for c in data_columns])\n",
        "\n",
        "    # Close the VALUES parenthesis\n",
        "    merge_statement += \")\"\n",
        "\n",
        "    # Return the dynamically built merge statement\n",
        "    return merge_statement"
      ],
      "metadata": {
        "id": "xKDm6__gGFfz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_statement =  dynamic_merge_statement_generator(source_temp_view, target_table, data_columns, primary_key_columns)\n",
        "print(merge_statement)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ILFDRA7pJR_",
        "outputId": "49078f0a-ac3c-4a55-c45c-675eab3651dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MERGE INTO demo_delta_cdf.product_sales_tgt target USING demo_delta_cdf_product_sales_src_view source ON target.product_id = source.product_id AND target.sales_date = source.sales_date WHEN MATCHED AND source._change_type = 'delete' THEN DELETE  WHEN NOT MATCHED THEN INSERT (product_id, sales_date, quantity, revenue) Values (source.product_id, source.sales_date, source.quantity, source.revenue)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Target Table before Second Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)\n",
        "spark.sql(merge_statement)\n",
        "print(\"Target Table after Second Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfk2ZZPXAIV1",
        "outputId": "ac6d580d-0e18-4b10-cc02-f19ef8703124"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Table before Second Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P002      |2025-08-01|5       |50.0   |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table after Second Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Update case in Source Table**"
      ],
      "metadata": {
        "id": "DzTndj7eFkLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Capture the current timestamp so that the CDC query will only fetch changes\n",
        "# that happen after this point (mimicking incremental processing).\n",
        "current_timestamp = spark.sql(\"SELECT current_timestamp\").collect()[0][0]\n",
        "\n",
        "# UPDATE sales for a product on a given day\n",
        "spark.sql(f\"UPDATE {source_table} SET quantity = 25, revenue = 250.0 WHERE product_id='P001' AND sales_date='2025-08-02'\")\n",
        "\n",
        "# Read the Change Data Feed (CDF) again, but this time starting from the captured timestamp.\n",
        "# This ensures we only capture the DELETE event and not the earlier inserts.\n",
        "cdc_df = (\n",
        "    spark.read.format(\"delta\")\n",
        "        .option(\"readChangeFeed\", \"true\")\n",
        "        .option(\"startingTimestamp\", current_timestamp)\n",
        "        .table(f\"{source_table}\")\n",
        ").filter(col('_change_type')!='update_preimage')\n",
        "\n",
        "# Register the CDF dataframe as a temporary view so it can be used in merge operations.\n",
        "cdc_df.createOrReplaceTempView(source_temp_view)\n",
        "\n",
        "# Display the captured CDC records.\n",
        "# This should show the delete event which can be used in the merge logic.\n",
        "spark.sql(f\"SELECT * FROM {source_temp_view}\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONBnnlEdAXYT",
        "outputId": "0f3946b8-2bb0-490a-dfd5-6b2214e50176"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+-------+----------------+---------------+----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type    |_commit_version|_commit_timestamp     |\n",
            "+----------+----------+--------+-------+----------------+---------------+----------------------+\n",
            "|P001      |2025-08-02|25      |250.0  |update_postimage|3              |2025-08-18 16:29:16.36|\n",
            "+----------+----------+--------+-------+----------------+---------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_merge_statement_generator(source_temp_view_name, target_delta_table_name, data_columns, primary_key_columns):\n",
        "    # Initialize an empty string to build the merge SQL statement\n",
        "    merge_statement = \"\"\n",
        "\n",
        "    # Begin the MERGE INTO statement with target table and source view\n",
        "    merge_statement += f\"\"\"MERGE INTO {target_delta_table_name} target USING {source_temp_view_name} source ON \"\"\"\n",
        "\n",
        "    # Create a list to hold join expressions between primary key columns of source and target\n",
        "    join_expression_list = []\n",
        "\n",
        "    # For each primary key column, build the equality condition (target.pk = source.pk)\n",
        "    for pk in primary_key_columns:\n",
        "        join_expression_list.append(f\"target.{pk} = source.{pk}\")\n",
        "\n",
        "    # Join all primary key conditions with \"AND\" to form the final join expression\n",
        "    join_expression = \" AND \".join(join_expression_list)\n",
        "\n",
        "    # Append the join expression to the merge statement\n",
        "    merge_statement += join_expression\n",
        "\n",
        "    # Add the SQL clause to handle update operations when the source record has _change_type = 'update_postimage'.\n",
        "    # This indicates that the target table should be updated with the latest values from the source table.\n",
        "    merge_statement += f\" WHEN MATCHED AND source._change_type = 'update_postimage' THEN UPDATE SET \"\n",
        "\n",
        "    # Dynamically build the \"SET\" part of the update by mapping each column in data_columns\n",
        "    # so that target.column = source.column (ensuring all specified columns are updated).\n",
        "    merge_statement += \", \".join([f\"{c} = source.{c}\" for c in data_columns])\n",
        "\n",
        "    ## Add delete clause for matched rows with change_type = delete\n",
        "    merge_statement += f\" WHEN MATCHED AND source._change_type = 'delete' THEN DELETE \"\n",
        "\n",
        "    # Convert the list of data columns into a comma-separated string for the INSERT part\n",
        "    data_columns_str = \", \".join(data_columns)\n",
        "\n",
        "    # Add the NOT MATCHED clause for insert operations, with column list\n",
        "    merge_statement += f\"\"\" WHEN NOT MATCHED THEN INSERT ({data_columns_str}) Values (\"\"\"\n",
        "\n",
        "    # Map each column in data_columns to its \"source.column_name\" counterpart\n",
        "    merge_statement += \", \".join([f\"source.{c}\" for c in data_columns])\n",
        "\n",
        "    # Close the VALUES parenthesis\n",
        "    merge_statement += \")\"\n",
        "\n",
        "    # Return the dynamically built merge statement\n",
        "    return merge_statement"
      ],
      "metadata": {
        "id": "5fnYoMJWUW5V"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_statement =  dynamic_merge_statement_generator(source_temp_view, target_table, data_columns, primary_key_columns)\n",
        "print(merge_statement)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFqZZY3Zpe1C",
        "outputId": "bd48c4ec-c8aa-46d7-979a-38798b54e76c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MERGE INTO demo_delta_cdf.product_sales_tgt target USING demo_delta_cdf_product_sales_src_view source ON target.product_id = source.product_id AND target.sales_date = source.sales_date WHEN MATCHED AND source._change_type = 'update_postimage' THEN UPDATE SET product_id = source.product_id, sales_date = source.sales_date, quantity = source.quantity, revenue = source.revenue WHEN MATCHED AND source._change_type = 'delete' THEN DELETE  WHEN NOT MATCHED THEN INSERT (product_id, sales_date, quantity, revenue) Values (source.product_id, source.sales_date, source.quantity, source.revenue)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Target Table before Third Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)\n",
        "spark.sql(merge_statement)\n",
        "print(\"Target Table after Third Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYwhoZXxCQ-f",
        "outputId": "0d1ea2a1-8b1b-4bfd-c9eb-97c0b9ac9283"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Table before Third Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table after Third Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling Corner Cases (Insert + Update in Same Batch)**"
      ],
      "metadata": {
        "id": "M92coLNwFz6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Capture the current timestamp so that the CDC query will only fetch changes\n",
        "# that happen after this point (mimicking incremental processing).\n",
        "current_timestamp = spark.sql(\"SELECT current_timestamp\").collect()[0][0]\n",
        "\n",
        "# UPDATE sales for a product on a given day\n",
        "spark.sql(f\"\"\"INSERT INTO {source_table} VALUES ('P009','2025-08-03',10,100.0)\"\"\");\n",
        "spark.sql(f\"UPDATE {source_table} SET quantity = 50, revenue = 250.0 WHERE product_id='P009' AND sales_date='2025-08-03'\")\n",
        "\n",
        "# Read the Change Data Feed (CDF) again, but this time starting from the captured timestamp.\n",
        "# This ensures we only capture the DELETE event and not the earlier inserts.\n",
        "cdc_df = (\n",
        "    spark.read.format(\"delta\")\n",
        "        .option(\"readChangeFeed\", \"true\")\n",
        "        .option(\"startingTimestamp\", current_timestamp)\n",
        "        .table(f\"{source_table}\")\n",
        ").filter(col('_change_type')!='update_preimage')\n",
        "\n",
        "# Register the CDF dataframe as a temporary view so it can be used in merge operations.\n",
        "cdc_df.createOrReplaceTempView(source_temp_view)\n",
        "\n",
        "# Display the captured CDC records.\n",
        "# This should show the delete event which can be used in the merge logic.\n",
        "spark.sql(f\"SELECT * FROM {source_temp_view}\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T6DI8WlZJec",
        "outputId": "92a3c004-b067-495f-a9ff-c5a5061f8a74"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type    |_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|P009      |2025-08-03|50      |250.0  |update_postimage|5              |2025-08-18 16:29:36.511|\n",
            "|P009      |2025-08-03|10      |100.0  |insert          |4              |2025-08-18 16:29:30.842|\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Target Table before Fourth Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)\n",
        "spark.sql(merge_statement)\n",
        "print(\"Target Table after Fourth Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFjMN_JKUG4J",
        "outputId": "66fd903c-3aa1-4785-a0ef-798340d403f1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Table before Fourth Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table after Fourth Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P009      |2025-08-03|50      |250.0  |\n",
            "|P009      |2025-08-03|10      |100.0  |\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rollback to previous version of Target Table as the previous case was handled wrongly**"
      ],
      "metadata": {
        "id": "S1_wOe00F9ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_last_version = spark.sql(f\"\"\"describe history {target_table}\"\"\").collect()[1]['version']\n",
        "print(\"The second last version for the target table is : \",second_last_version)\n",
        "spark.sql(f\"\"\"RESTORE TABLE {target_table} TO VERSION AS OF {second_last_version}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzCNbuiRYdS2",
        "outputId": "b5eaccb4-6fd7-406c-f65e-238fe7c4da3c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The second last version for the target table is :  3\n",
            "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
            "|table_size_after_restore|num_of_files_after_restore|num_removed_files|num_restored_files|removed_files_size|restored_files_size|\n",
            "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
            "|2506                    |2                         |1                |0                 |1308              |0                  |\n",
            "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KofR-izWYt6Y",
        "outputId": "2d5f3959-2c13-4b6c-bcd8-51187612865d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get only latest state of a record from cdc_df**"
      ],
      "metadata": {
        "id": "t9UEbJFWIBJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "print(\"Initial CDC df : \")\n",
        "cdc_df.show(truncate=False)\n",
        "# Deduplicate by composite key (product_id + sales_date)\n",
        "w = Window.partitionBy(*[col(c) for c in primary_key_columns]) \\\n",
        "          .orderBy(col(\"_commit_version\").desc(), col(\"_commit_timestamp\").desc())\n",
        "\n",
        "cdc_df = (\n",
        "    cdc_df\n",
        "      .withColumn(\"_rn\", row_number().over(w))\n",
        "      .filter(col(\"_rn\") == 1)\n",
        "      .drop(\"_rn\")\n",
        ")\n",
        "\n",
        "cdc_df.createOrReplaceTempView(source_temp_view)\n",
        "print(\"After taking only latest rows : \")\n",
        "spark.sql(f\"\"\"select * from {source_temp_view}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKVQY2LjY3_2",
        "outputId": "033420e1-cb52-4464-9979-af96f136d042"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial CDC df : \n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type    |_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|P009      |2025-08-03|50      |250.0  |update_postimage|5              |2025-08-18 16:29:36.511|\n",
            "|P009      |2025-08-03|10      |100.0  |insert          |4              |2025-08-18 16:29:30.842|\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "\n",
            "After taking only latest rows : \n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type    |_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|P009      |2025-08-03|50      |250.0  |update_postimage|5              |2025-08-18 16:29:36.511|\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Current Source Data -- \")\n",
        "spark.sql(f\"\"\"select * from {source_table}\"\"\").show(truncate=False)\n",
        "print(\"Target Table before Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)\n",
        "spark.sql(merge_statement)\n",
        "print(\"Target Table after Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCl6XCx4ZLUW",
        "outputId": "a074281c-4da0-4ef9-d26d-749aa93bdd47"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Source Data -- \n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P009      |2025-08-03|50      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table before Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table after Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P009      |2025-08-03|50      |250.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling Corner Cases (Insert + Update + Delete in Same Batch)**"
      ],
      "metadata": {
        "id": "qXIm6oMMM6zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Capture the current timestamp so that the CDC query will only fetch changes\n",
        "# that happen after this point (mimicking incremental processing).\n",
        "current_timestamp = spark.sql(\"SELECT current_timestamp\").collect()[0][0]\n",
        "\n",
        "# Insert, Update, Delete a row\n",
        "spark.sql(f\"\"\"INSERT INTO {source_table} VALUES ('P010','2025-08-04',80,120.0)\"\"\");\n",
        "spark.sql(f\"UPDATE {source_table} SET quantity = 50, revenue = 210.0 WHERE product_id='P010' AND sales_date='2025-08-04'\");\n",
        "spark.sql(f\"\"\"Delete from {source_table} WHERE product_id='P010' AND sales_date='2025-08-04'\"\"\");\n",
        "\n",
        "# Read the Change Data Feed (CDF) again, but this time starting from the captured timestamp.\n",
        "# This ensures we only capture the DELETE event and not the earlier inserts.\n",
        "cdc_df = (\n",
        "    spark.read.format(\"delta\")\n",
        "        .option(\"readChangeFeed\", \"true\")\n",
        "        .option(\"startingTimestamp\", current_timestamp)\n",
        "        .table(f\"{source_table}\")\n",
        ").filter(col('_change_type')!='update_preimage')\n",
        "\n",
        "# Register the CDF dataframe as a temporary view so it can be used in merge operations.\n",
        "cdc_df.createOrReplaceTempView(source_temp_view)\n",
        "\n",
        "# Display the captured CDC records.\n",
        "# This should show all the insert, update, delete events whatever happened in Source Table...\n",
        "spark.sql(f\"SELECT * FROM {source_temp_view}\").show(truncate=False)\n",
        "\n",
        "# Deduplicate by composite key (product_id + sales_date)\n",
        "w = Window.partitionBy(*[col(c) for c in primary_key_columns]) \\\n",
        "          .orderBy(col(\"_commit_version\").desc(), col(\"_commit_timestamp\").desc())\n",
        "\n",
        "cdc_df = (\n",
        "    cdc_df\n",
        "      .withColumn(\"_rn\", row_number().over(w))\n",
        "      .filter(col(\"_rn\") == 1)\n",
        "      .drop(\"_rn\")\n",
        ")\n",
        "\n",
        "cdc_df.createOrReplaceTempView(source_temp_view)\n",
        "print(\"After taking only latest rows : \")\n",
        "spark.sql(f\"\"\"select * from {source_temp_view}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "id": "Nkx3SBk3M8i4",
        "outputId": "546f72bc-4ac0-4f0a-e38c-4079b2d40294",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type    |_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|P010      |2025-08-04|50      |210.0  |update_postimage|7              |2025-08-18 16:30:48.371|\n",
            "|P010      |2025-08-04|50      |210.0  |delete          |8              |2025-08-18 16:30:52.92 |\n",
            "|P010      |2025-08-04|80      |120.0  |insert          |6              |2025-08-18 16:30:42.471|\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "\n",
            "After taking only latest rows : \n",
            "+----------+----------+--------+-------+------------+---------------+----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type|_commit_version|_commit_timestamp     |\n",
            "+----------+----------+--------+-------+------------+---------------+----------------------+\n",
            "|P010      |2025-08-04|50      |210.0  |delete      |8              |2025-08-18 16:30:52.92|\n",
            "+----------+----------+--------+-------+------------+---------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Current Source Data -- \")\n",
        "spark.sql(f\"\"\"select * from {source_table}\"\"\").show(truncate=False)\n",
        "print(\"Target Table before Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)\n",
        "spark.sql(merge_statement)\n",
        "print(\"Target Table after Merge Statement execution...\")\n",
        "spark.sql(f\"\"\"select * from {target_table}\"\"\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVomgIoPZL8-",
        "outputId": "7d12c8b9-fd9d-475c-d3e7-66200c3aa037"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Source Data -- \n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P009      |2025-08-03|50      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table before Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P009      |2025-08-03|50      |250.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table after Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P010      |2025-08-04|50      |210.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P009      |2025-08-03|50      |250.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_merge_statement_generator(source_temp_view_name, target_delta_table_name, data_columns, primary_key_columns):\n",
        "    # Initialize an empty string to build the merge SQL statement\n",
        "    merge_statement = \"\"\n",
        "\n",
        "    # Begin the MERGE INTO statement with target table and source view\n",
        "    merge_statement += f\"\"\"MERGE INTO {target_delta_table_name} target USING {source_temp_view_name} source ON \"\"\"\n",
        "\n",
        "    # Create a list to hold join expressions between primary key columns of source and target\n",
        "    join_expression_list = []\n",
        "\n",
        "    # For each primary key column, build the equality condition (target.pk = source.pk)\n",
        "    for pk in primary_key_columns:\n",
        "        join_expression_list.append(f\"target.{pk} = source.{pk}\")\n",
        "\n",
        "    # Join all primary key conditions with \"AND\" to form the final join expression\n",
        "    join_expression = \" AND \".join(join_expression_list)\n",
        "\n",
        "    # Append the join expression to the merge statement\n",
        "    merge_statement += join_expression\n",
        "\n",
        "    # Add the SQL clause to handle update operations when the source record has _change_type = 'update_postimage'.\n",
        "    # This indicates that the target table should be updated with the latest values from the source table.\n",
        "    merge_statement += f\" WHEN MATCHED AND source._change_type = 'update_postimage' THEN UPDATE SET \"\n",
        "\n",
        "    # Dynamically build the \"SET\" part of the update by mapping each column in data_columns\n",
        "    # so that target.column = source.column (ensuring all specified columns are updated).\n",
        "    merge_statement += \", \".join([f\"{c} = source.{c}\" for c in data_columns])\n",
        "\n",
        "    ## Add delete clause for matched rows with change_type = delete\n",
        "    merge_statement += f\" WHEN MATCHED AND source._change_type = 'delete' THEN DELETE \"\n",
        "\n",
        "    # Convert the list of data columns into a comma-separated string for the INSERT part\n",
        "    data_columns_str = \", \".join(data_columns)\n",
        "\n",
        "    # Add the NOT MATCHED clause for insert(& handling corner cases) operations, with column list\n",
        "    merge_statement += f\"\"\" WHEN NOT MATCHED AND source._change_type IN ('insert','update_postimage') THEN INSERT ({data_columns_str}) Values (\"\"\"\n",
        "\n",
        "    # Map each column in data_columns to its \"source.column_name\" counterpart\n",
        "    merge_statement += \", \".join([f\"source.{c}\" for c in data_columns])\n",
        "\n",
        "    # Close the VALUES parenthesis\n",
        "    merge_statement += \")\"\n",
        "\n",
        "    # Return the dynamically built merge statement\n",
        "    return merge_statement"
      ],
      "metadata": {
        "id": "vNmiZZk_QBPf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# Step 1: Rollback the Target Table to its previous version\n",
        "# ---------------------------------------------------------\n",
        "# Fetch the second last version from the Delta table history\n",
        "second_last_version = spark.sql(f\"DESCRIBE HISTORY {target_table}\").collect()[1]['version']\n",
        "print(\"The second last version for the target table is :\", second_last_version)\n",
        "\n",
        "# Restore the target table to the second last version\n",
        "spark.sql(f\"RESTORE TABLE {target_table} TO VERSION AS OF {second_last_version}\").show(truncate=False)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 2: Generate the dynamic MERGE SQL using our function\n",
        "# ---------------------------------------------------------\n",
        "merge_statement = dynamic_merge_statement_generator(\n",
        "    source_temp_view,\n",
        "    target_table,\n",
        "    data_columns,\n",
        "    primary_key_columns\n",
        ")\n",
        "\n",
        "print(\"Generated Merge Statement:\\n\", merge_statement)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 3: Check current state of source and target before merge\n",
        "# ---------------------------------------------------------\n",
        "print(\"Current Source Data -- \")\n",
        "spark.sql(f\"SELECT * FROM {source_table}\").show(truncate=False)\n",
        "\n",
        "print(\"Current CDC Data -- \")\n",
        "spark.sql(f\"SELECT * FROM {source_temp_view}\").show(truncate=False)\n",
        "\n",
        "\n",
        "print(\"Target Table BEFORE Merge Statement execution...\")\n",
        "spark.sql(f\"SELECT * FROM {target_table}\").show(truncate=False)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 4: Execute the dynamic MERGE statement\n",
        "# ---------------------------------------------------------\n",
        "spark.sql(merge_statement)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 5: Validate results after merge\n",
        "# ---------------------------------------------------------\n",
        "print(\"Target Table AFTER Merge Statement execution...\")\n",
        "spark.sql(f\"SELECT * FROM {target_table}\").show(truncate=False)\n"
      ],
      "metadata": {
        "id": "Tbmzx_uVQOUQ",
        "outputId": "49d58459-0b8d-4a82-b7bc-0f976cb95cb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The second last version for the target table is : 6\n",
            "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
            "|table_size_after_restore|num_of_files_after_restore|num_removed_files|num_restored_files|removed_files_size|restored_files_size|\n",
            "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
            "|3734                    |3                         |1                |0                 |1228              |0                  |\n",
            "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
            "\n",
            "Generated Merge Statement:\n",
            " MERGE INTO demo_delta_cdf.product_sales_tgt target USING demo_delta_cdf_product_sales_src_view source ON target.product_id = source.product_id AND target.sales_date = source.sales_date WHEN MATCHED AND source._change_type = 'update_postimage' THEN UPDATE SET product_id = source.product_id, sales_date = source.sales_date, quantity = source.quantity, revenue = source.revenue WHEN MATCHED AND source._change_type = 'delete' THEN DELETE  WHEN NOT MATCHED AND source._change_type IN ('insert','update_postimage') THEN INSERT (product_id, sales_date, quantity, revenue) Values (source.product_id, source.sales_date, source.quantity, source.revenue)\n",
            "Current Source Data -- \n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P009      |2025-08-03|50      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Current CDC Data -- \n",
            "+----------+----------+--------+-------+------------+---------------+----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type|_commit_version|_commit_timestamp     |\n",
            "+----------+----------+--------+-------+------------+---------------+----------------------+\n",
            "|P010      |2025-08-04|50      |210.0  |delete      |8              |2025-08-18 16:30:52.92|\n",
            "+----------+----------+--------+-------+------------+---------------+----------------------+\n",
            "\n",
            "Target Table BEFORE Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P009      |2025-08-03|50      |250.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table AFTER Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|25      |250.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P009      |2025-08-03|50      |250.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Complete Flow**"
      ],
      "metadata": {
        "id": "8WDhSA5RULGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "import time\n",
        "\n",
        "#create a database demo_delta_cdf to organize our tables.\n",
        "db = \"demo_delta_cdf\"\n",
        "spark.sql(f\"Drop DATABASE if exists {db} CASCADE \")\n",
        "spark.sql(f\"CREATE DATABASE  {db}\")\n",
        "\n",
        "source_table = f\"{db}.product_sales_src\"\n",
        "target_table = f\"{db}.product_sales_tgt\"\n",
        "\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {source_table}\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {target_table}\")\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "CREATE TABLE {source_table} (\n",
        "  product_id   STRING,\n",
        "  sales_date   DATE,\n",
        "  quantity     INT,\n",
        "  revenue      DOUBLE\n",
        ") using Delta TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
        "-- CDF is enabled on this table so that any inserts, updates, or deletes can be captured\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "CREATE TABLE {target_table} (\n",
        "  product_id   STRING,\n",
        "  sales_date   DATE,\n",
        "  quantity     INT,\n",
        "  revenue      DOUBLE\n",
        ") using Delta\n",
        "\"\"\")\n",
        "\n",
        "data_columns = [\"product_id\", \"sales_date\", \"quantity\", \"revenue\"]\n",
        "primary_key_columns = [\"product_id\", \"sales_date\"]\n",
        "\n",
        "# Helper: view name convention (same as source with '.' -> '_' and suffix _view)\n",
        "def to_temp_view_name(table_name: str) -> str:\n",
        "    return f\"{table_name}_view\".replace(\".\", \"_\")\n",
        "\n",
        "# Helper: symmetric diff validation (returns True if identical)\n",
        "def validate_target_equals_source(cols, src_table, tgt_table):\n",
        "    cols_list = \", \".join(cols)\n",
        "    diff_sql = f\"\"\"\n",
        "    (SELECT {cols_list} FROM {src_table}\n",
        "     EXCEPT\n",
        "     SELECT {cols_list} FROM {tgt_table})\n",
        "    UNION\n",
        "    (SELECT {cols_list} FROM {tgt_table}\n",
        "     EXCEPT\n",
        "     SELECT {cols_list} FROM {src_table})\n",
        "    \"\"\"\n",
        "    diff_count = spark.sql(diff_sql).count()\n",
        "    return diff_count == 0\n",
        "\n",
        "# Helper: read CDF between start_ts (string) and now, filter out preimage rows\n",
        "def read_cdf_and_dedupe(source_table, start_ts, pk_cols, temp_view_name):\n",
        "    # Read CDF since starting timestamp\n",
        "    cdc_df = (\n",
        "        spark.read.format(\"delta\")\n",
        "            .option(\"readChangeFeed\", \"true\")\n",
        "            .option(\"startingTimestamp\", str(start_ts))\n",
        "            .table(source_table)\n",
        "    )\n",
        "    print(\"Current CDC Data : \")\n",
        "    cdc_df.show(truncate=False)\n",
        "\n",
        "    cdc_df = cdc_df.filter(col(\"_change_type\") != \"update_preimage\")\n",
        "\n",
        "    # If there's nothing, create an empty view and return df\n",
        "    if len(cdc_df.columns) == 0 or cdc_df.count() == 0:\n",
        "        # create empty temp view with expected schema (no rows)\n",
        "        cdc_df.createOrReplaceTempView(temp_view_name)\n",
        "        return\n",
        "\n",
        "    # Deduplicate: keep only latest change per composite key\n",
        "    w = Window.partitionBy(*[col(c) for c in pk_cols]) \\\n",
        "              .orderBy(col(\"_commit_version\").desc(), col(\"_commit_timestamp\").desc())\n",
        "\n",
        "    cdc_df_latest = (\n",
        "        cdc_df.withColumn(\"_rn\", row_number().over(w))\n",
        "              .filter(col(\"_rn\") == 1)\n",
        "              .drop(\"_rn\")\n",
        "    )\n",
        "\n",
        "    # Register temp view for SQL merge\n",
        "    cdc_df_latest.createOrReplaceTempView(temp_view_name)\n",
        "    return\n",
        "\n",
        "# Helper: get current timestamp string from Spark\n",
        "def current_ts_str():\n",
        "    return spark.sql(\"SELECT current_timestamp() AS ts\").collect()[0][\"ts\"]\n",
        "\n",
        "def dynamic_merge_statement_generator(source_temp_view_name, target_delta_table_name, data_columns, primary_key_columns):\n",
        "    # Initialize an empty string to build the merge SQL statement\n",
        "    merge_statement = \"\"\n",
        "\n",
        "    # Begin the MERGE INTO statement with target table and source view\n",
        "    merge_statement += f\"\"\"MERGE INTO {target_delta_table_name} target USING {source_temp_view_name} source ON \"\"\"\n",
        "\n",
        "    # Create a list to hold join expressions between primary key columns of source and target\n",
        "    join_expression_list = []\n",
        "\n",
        "    # For each primary key column, build the equality condition (target.pk = source.pk)\n",
        "    for pk in primary_key_columns:\n",
        "        join_expression_list.append(f\"target.{pk} = source.{pk}\")\n",
        "\n",
        "    # Join all primary key conditions with \"AND\" to form the final join expression\n",
        "    join_expression = \" AND \".join(join_expression_list)\n",
        "\n",
        "    # Append the join expression to the merge statement\n",
        "    merge_statement += join_expression\n",
        "\n",
        "    # Add the SQL clause to handle update operations when the source record has _change_type = 'update_postimage'.\n",
        "    # This indicates that the target table should be updated with the latest values from the source table.\n",
        "    merge_statement += f\" WHEN MATCHED AND source._change_type = 'update_postimage' THEN UPDATE SET \"\n",
        "\n",
        "    # Dynamically build the \"SET\" part of the update by mapping each column in data_columns\n",
        "    # so that target.column = source.column (ensuring all specified columns are updated).\n",
        "    merge_statement += \", \".join([f\"{c} = source.{c}\" for c in data_columns])\n",
        "\n",
        "    ## Add delete clause for matched rows with change_type = delete\n",
        "    merge_statement += f\" WHEN MATCHED AND source._change_type = 'delete' THEN DELETE \"\n",
        "\n",
        "    # Convert the list of data columns into a comma-separated string for the INSERT part\n",
        "    data_columns_str = \", \".join(data_columns)\n",
        "\n",
        "    # Add the NOT MATCHED clause for insert(& handling corner cases) operations, with column list\n",
        "    merge_statement += f\"\"\" WHEN NOT MATCHED AND source._change_type IN ('insert','update_postimage') THEN INSERT ({data_columns_str}) Values (\"\"\"\n",
        "\n",
        "    # Map each column in data_columns to its \"source.column_name\" counterpart\n",
        "    merge_statement += \", \".join([f\"source.{c}\" for c in data_columns])\n",
        "\n",
        "    # Close the VALUES parenthesis\n",
        "    merge_statement += \")\"\n",
        "\n",
        "    # Return the dynamically built merge statement\n",
        "    return merge_statement\n",
        "\n",
        "print(\"Seeding initial data into source...\")\n",
        "spark.sql(f\"\"\"\n",
        "INSERT INTO {source_table} VALUES\n",
        "('P001','2025-08-01',10,100.0),\n",
        "('P001','2025-08-02',20,200.0),\n",
        "('P002','2025-08-01',5,50.0),\n",
        "('P003','2025-08-01',7,70.0)\n",
        "\"\"\")\n",
        "\n",
        "source_temp_view = to_temp_view_name(source_table)\n",
        "\n",
        "# Initial full CDF read from epoch (first load)\n",
        "print('*'*100)\n",
        "\n",
        "print(\"Current Source Data -- \")\n",
        "spark.sql(f\"SELECT * FROM {source_table}\").show(truncate=False)\n",
        "\n",
        "cdc_df = read_cdf_and_dedupe(source_table, \"1970-01-01 00:00:00\", primary_key_columns, source_temp_view)\n",
        "print(\"CDC Data after taking latest state only-- \")\n",
        "spark.sql(f\"SELECT * FROM {source_temp_view}\").show(truncate=False)\n",
        "\n",
        "print(\"Target Table BEFORE Merge Statement execution...\")\n",
        "spark.sql(f\"SELECT * FROM {target_table}\").show(truncate=False)\n",
        "\n",
        "merge_statement = dynamic_merge_statement_generator(source_temp_view, target_table, data_columns, primary_key_columns)\n",
        "spark.sql(merge_statement)\n",
        "\n",
        "print(\"Target Table AFTER Merge Statement execution...\")\n",
        "spark.sql(f\"SELECT * FROM {target_table}\").show(truncate=False)\n",
        "\n",
        "assert validate_target_equals_source(data_columns, source_table, target_table), \"Initial load mismatch!\"\n",
        "print(\"Initial load validated: Target equals Source\")\n",
        "print('*'*100)\n",
        "\n",
        "# Capture checkpoint timestamp BEFORE changes\n",
        "batch_a_start_ts = current_ts_str()\n",
        "time.sleep(0.5)  # small wait to ensure different commit timestamp\n",
        "\n",
        "print(\"\\n--- Running Batch A (mixed operations, some inserts, some insert->update, some insert->update->delete) ---\")\n",
        "# Batch A operations: some inserts, some insert->update, some insert->update->delete\n",
        "#  - P004: insert only\n",
        "#  - P005: insert then update\n",
        "#  - P006: insert then update then delete\n",
        "spark.sql(f\"INSERT INTO {source_table} VALUES ('P004','2025-08-01',12,120.0)\")                # insert only\n",
        "spark.sql(f\"INSERT INTO {source_table} VALUES ('P005','2025-08-01',8,80.0)\")                  # insert\n",
        "spark.sql(f\"UPDATE {source_table} SET quantity = 18, revenue = 180.0 WHERE product_id='P005' AND sales_date='2025-08-01'\")  # update\n",
        "spark.sql(f\"INSERT INTO {source_table} VALUES ('P006','2025-08-01',3,30.0)\")  # insert\n",
        "spark.sql(f\"UPDATE {source_table} SET quantity = 4, revenue = 40.0 WHERE product_id='P006' AND sales_date='2025-08-01'\")# update\n",
        "spark.sql(f\"DELETE FROM {source_table} WHERE product_id='P006' AND sales_date='2025-08-01'\")# delete\n",
        "\n",
        "\n",
        "\n",
        "print(\"Current Source Data -- \")\n",
        "spark.sql(f\"SELECT * FROM {source_table}\").show(truncate=False)\n",
        "\n",
        "cdc_df = read_cdf_and_dedupe(source_table, batch_a_start_ts, primary_key_columns, source_temp_view)\n",
        "print(\"CDC Data after taking latest state only-- \")\n",
        "spark.sql(f\"SELECT * FROM {source_temp_view}\").show(truncate=False)\n",
        "\n",
        "print(\"Target Table BEFORE Merge Statement execution...\")\n",
        "spark.sql(f\"SELECT * FROM {target_table}\").show(truncate=False)\n",
        "\n",
        "spark.sql(merge_statement)\n",
        "\n",
        "print(\"Target Table AFTER Merge Statement execution...\")\n",
        "spark.sql(f\"SELECT * FROM {target_table}\").show(truncate=False)\n",
        "\n",
        "assert validate_target_equals_source(data_columns, source_table, target_table), \"Initial load mismatch!\"\n",
        "print(\"Initial load validated: Target equals Source\")\n",
        "print('*'*100)\n",
        "\n",
        "# Batch B ops:\n",
        "# - Update existing P001 on 2025-08-01\n",
        "# - Delete existing P002 on 2025-08-01\n",
        "# - Insert new P007, P008\n",
        "print(\"\\n--- Running Batch B (update existing, delete existing, new inserts) ---\")\n",
        "batch_b_start_ts = current_ts_str()\n",
        "time.sleep(0.5)\n",
        "spark.sql(f\"UPDATE {source_table} SET quantity = 15, revenue = 150.0 WHERE product_id='P001' AND sales_date='2025-08-01'\")\n",
        "spark.sql(f\"DELETE FROM {source_table} WHERE product_id='P002' AND sales_date='2025-08-01'\")\n",
        "spark.sql(f\"INSERT INTO {source_table} VALUES ('P007','2025-08-01',9,90.0)\")\n",
        "spark.sql(f\"INSERT INTO {source_table} VALUES ('P008','2025-08-02',2,20.0)\")\n",
        "\n",
        "\n",
        "print(\"Current Source Data -- \")\n",
        "spark.sql(f\"SELECT * FROM {source_table}\").show(truncate=False)\n",
        "\n",
        "cdc_df = read_cdf_and_dedupe(source_table, batch_b_start_ts, primary_key_columns, source_temp_view)\n",
        "print(\"CDC Data after taking latest state only-- \")\n",
        "spark.sql(f\"SELECT * FROM {source_temp_view}\").show(truncate=False)\n",
        "print(\"Target Table BEFORE Merge Statement execution...\")\n",
        "spark.sql(f\"SELECT * FROM {target_table}\").show(truncate=False)\n",
        "spark.sql(merge_statement)\n",
        "print(\"Target Table AFTER Merge Statement execution...\")\n",
        "spark.sql(f\"SELECT * FROM {target_table}\").show(truncate=False)\n",
        "\n",
        "assert validate_target_equals_source(data_columns, source_table, target_table), \"Initial load mismatch!\"\n",
        "print(\"Initial load validated: Target equals Source\")\n",
        "print('*'*100)\n"
      ],
      "metadata": {
        "id": "0EdZ8jeaUMuS",
        "outputId": "e42b9ebf-f198-4a68-8e3d-1b929f3d71bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seeding initial data into source...\n",
            "****************************************************************************************************\n",
            "Current Source Data -- \n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P002      |2025-08-01|5       |50.0   |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Current CDC Data : \n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type|_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "|P001      |2025-08-01|10      |100.0  |insert      |1              |2025-08-18 16:31:56.629|\n",
            "|P001      |2025-08-02|20      |200.0  |insert      |1              |2025-08-18 16:31:56.629|\n",
            "|P002      |2025-08-01|5       |50.0   |insert      |1              |2025-08-18 16:31:56.629|\n",
            "|P003      |2025-08-01|7       |70.0   |insert      |1              |2025-08-18 16:31:56.629|\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "\n",
            "CDC Data after taking latest state only-- \n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type|_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "|P001      |2025-08-01|10      |100.0  |insert      |1              |2025-08-18 16:31:56.629|\n",
            "|P001      |2025-08-02|20      |200.0  |insert      |1              |2025-08-18 16:31:56.629|\n",
            "|P002      |2025-08-01|5       |50.0   |insert      |1              |2025-08-18 16:31:56.629|\n",
            "|P003      |2025-08-01|7       |70.0   |insert      |1              |2025-08-18 16:31:56.629|\n",
            "+----------+----------+--------+-------+------------+---------------+-----------------------+\n",
            "\n",
            "Target Table BEFORE Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table AFTER Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P002      |2025-08-01|5       |50.0   |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Initial load validated: Target equals Source\n",
            "****************************************************************************************************\n",
            "\n",
            "--- Running Batch A (mixed operations, some inserts, some insert->update, some insert->update->delete) ---\n",
            "Current Source Data -- \n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P005      |2025-08-01|18      |180.0  |\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P002      |2025-08-01|5       |50.0   |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P004      |2025-08-01|12      |120.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Current CDC Data : \n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type    |_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|P005      |2025-08-01|8       |80.0   |update_preimage |4              |2025-08-18 16:32:29.702|\n",
            "|P005      |2025-08-01|18      |180.0  |update_postimage|4              |2025-08-18 16:32:29.702|\n",
            "|P006      |2025-08-01|3       |30.0   |update_preimage |6              |2025-08-18 16:32:37.729|\n",
            "|P006      |2025-08-01|4       |40.0   |update_postimage|6              |2025-08-18 16:32:37.729|\n",
            "|P006      |2025-08-01|4       |40.0   |delete          |7              |2025-08-18 16:32:41.413|\n",
            "|P005      |2025-08-01|8       |80.0   |insert          |3              |2025-08-18 16:32:25.031|\n",
            "|P006      |2025-08-01|3       |30.0   |insert          |5              |2025-08-18 16:32:33.619|\n",
            "|P004      |2025-08-01|12      |120.0  |insert          |2              |2025-08-18 16:32:22.257|\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "\n",
            "CDC Data after taking latest state only-- \n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type    |_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|P004      |2025-08-01|12      |120.0  |insert          |2              |2025-08-18 16:32:22.257|\n",
            "|P005      |2025-08-01|18      |180.0  |update_postimage|4              |2025-08-18 16:32:29.702|\n",
            "|P006      |2025-08-01|4       |40.0   |delete          |7              |2025-08-18 16:32:41.413|\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "\n",
            "Target Table BEFORE Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P002      |2025-08-01|5       |50.0   |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table AFTER Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P002      |2025-08-01|5       |50.0   |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P004      |2025-08-01|12      |120.0  |\n",
            "|P005      |2025-08-01|18      |180.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Initial load validated: Target equals Source\n",
            "****************************************************************************************************\n",
            "\n",
            "--- Running Batch B (update existing, delete existing, new inserts) ---\n",
            "Current Source Data -- \n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|15      |150.0  |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P005      |2025-08-01|18      |180.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P007      |2025-08-01|9       |90.0   |\n",
            "|P008      |2025-08-02|2       |20.0   |\n",
            "|P004      |2025-08-01|12      |120.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Current CDC Data : \n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type    |_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|P001      |2025-08-01|10      |100.0  |update_preimage |8              |2025-08-18 16:33:08.36 |\n",
            "|P001      |2025-08-01|15      |150.0  |update_postimage|8              |2025-08-18 16:33:08.36 |\n",
            "|P002      |2025-08-01|5       |50.0   |delete          |9              |2025-08-18 16:33:13.375|\n",
            "|P007      |2025-08-01|9       |90.0   |insert          |10             |2025-08-18 16:33:15.931|\n",
            "|P008      |2025-08-02|2       |20.0   |insert          |11             |2025-08-18 16:33:21.291|\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "\n",
            "CDC Data after taking latest state only-- \n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|product_id|sales_date|quantity|revenue|_change_type    |_commit_version|_commit_timestamp      |\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "|P001      |2025-08-01|15      |150.0  |update_postimage|8              |2025-08-18 16:33:08.36 |\n",
            "|P002      |2025-08-01|5       |50.0   |delete          |9              |2025-08-18 16:33:13.375|\n",
            "|P007      |2025-08-01|9       |90.0   |insert          |10             |2025-08-18 16:33:15.931|\n",
            "|P008      |2025-08-02|2       |20.0   |insert          |11             |2025-08-18 16:33:21.291|\n",
            "+----------+----------+--------+-------+----------------+---------------+-----------------------+\n",
            "\n",
            "Target Table BEFORE Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P001      |2025-08-01|10      |100.0  |\n",
            "|P002      |2025-08-01|5       |50.0   |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P004      |2025-08-01|12      |120.0  |\n",
            "|P005      |2025-08-01|18      |180.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Target Table AFTER Merge Statement execution...\n",
            "+----------+----------+--------+-------+\n",
            "|product_id|sales_date|quantity|revenue|\n",
            "+----------+----------+--------+-------+\n",
            "|P001      |2025-08-01|15      |150.0  |\n",
            "|P001      |2025-08-02|20      |200.0  |\n",
            "|P003      |2025-08-01|7       |70.0   |\n",
            "|P007      |2025-08-01|9       |90.0   |\n",
            "|P008      |2025-08-02|2       |20.0   |\n",
            "|P004      |2025-08-01|12      |120.0  |\n",
            "|P005      |2025-08-01|18      |180.0  |\n",
            "+----------+----------+--------+-------+\n",
            "\n",
            "Initial load validated: Target equals Source\n",
            "****************************************************************************************************\n"
          ]
        }
      ]
    }
  ]
}