{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Configure SparkSession with Delta Lake configuration",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%session_id_prefix native-delta-sql-\n%glue_version 4.0\n%idle_timeout 60\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n  \"--datalake-formats\": \"delta\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nSetting session ID prefix to native-delta-sql-\nSetting Glue version to: 4.0\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog', '--datalake-formats': 'delta'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Create a DataBase in Glue Data Catalog for experiment",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%%sql\nCREATE DATABASE IF NOT EXISTS deltalake_db",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: d69eb451-cebd-44c9-bc52-eabe66487201\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session d69eb451-cebd-44c9-bc52-eabe66487201 to get into ready status...\nSession d69eb451-cebd-44c9-bc52-eabe66487201 has been created.\n++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Create a CSV Table pointing to popular Iris Dataset",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "Dataset Download link: https://www.kaggle.com/datasets/saurabh00007/iriscsv",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"\"\"\nCREATE TABLE deltalake_db.csv_table (\n  id INT,\n  SEPAL_LENGTH STRING,\n  SEPAL_WIDTH STRING,\n  PETAL_LENGTH STRING,\n  PETAL_WIDTH STRING,\n  CLASS_NAME STRING\n)\nUSING csv\nOPTIONS (\n  path 's3://deltasparktesting/iris_data/',\n  header 'true',\n  inferSchema 'true',\n  delimiter ','\n)\n\"\"\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Inspect the CSV Table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%%sql\nselect * from deltalake_db.csv_table limit 10;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+------------+-----------+------------+-----------+-----------+\n| id|SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH| CLASS_NAME|\n+---+------------+-----------+------------+-----------+-----------+\n|100|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n|101|         4.9|          3|         1.4|        0.2|Iris-setosa|\n|102|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n|103|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n|104|           5|        3.6|         1.4|        0.2|Iris-setosa|\n|105|         5.4|        3.9|         1.7|        0.4|Iris-setosa|\n|106|         4.6|        3.4|         1.4|        0.3|Iris-setosa|\n|107|           5|        3.4|         1.5|        0.2|Iris-setosa|\n|108|         4.4|        2.9|         1.4|      0.285|Iris-setosa|\n|109|         4.9|        3.1|         1.5|        0.1|Iris-setosa|\n+---+------------+-----------+------------+-----------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Distinct classes available in the CSV Table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%%sql\nselect distinct CLASS_NAME from deltalake_db.csv_table limit 10;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+\n|      CLASS_NAME|\n+----------------+\n|  Iris-virginica|\n|     Iris-setosa|\n| Iris-versicolor|\n|another-category|\n+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Create the Delta Table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# create table in metastore\nquery = f\"\"\"\nCREATE TABLE IF NOT EXISTS deltalake_db.delta_table (\n  ID INT,\n  SEPAL_LENGTH STRING,\n  SEPAL_WIDTH STRING,\n  PETAL_LENGTH STRING,\n  PETAL_WIDTH STRING,\n  CLASS_NAME STRING\n)\nUSING delta\nLOCATION 's3://deltasparktesting/iris_data_delta/'\n\"\"\"\n\nspark.sql(query)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Check the created tables",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%%sql\nuse deltalake_db",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nSHOW TABLES",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------+-----------+-----------+\n|   namespace|  tableName|isTemporary|\n+------------+-----------+-----------+\n|deltalake_db|  csv_table|      false|\n|deltalake_db|delta_table|      false|\n+------------+-----------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Inspect the DeltaLake Table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%%sql # Read table from metastore\nSELECT * FROM deltalake_db.delta_table;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+------------+-----------+------------+-----------+----------+\n| ID|SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH|CLASS_NAME|\n+---+------------+-----------+------------+-----------+----------+\n+---+------------+-----------+------------+-----------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Ingest the data in DeltaLake",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%%sql\ninsert into deltalake_db.delta_table\nselect * from deltalake_db.csv_table;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# The concept used to transform CSV Files to Delta is explained in below video in-depth",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "https://youtu.be/mPkXQtE0fkQ?si=JZJc1_5jaVNMu-ru",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "# Inspect the Delta Lake Table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%%sql # Read table from metastore\nSELECT * FROM deltalake_db.delta_table limit 10;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+------------+-----------+------------+-----------+--------------+\n| ID|SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH|    CLASS_NAME|\n+---+------------+-----------+------------+-----------+--------------+\n|  1|         6.3|        3.3|           6|        2.5|Iris-virginica|\n|  2|         5.8|        2.7|         5.1|        1.9|Iris-virginica|\n|  3|         7.1|          3|         5.9|        2.1|Iris-virginica|\n|  4|         6.3|        2.9|         5.6|        1.8|Iris-virginica|\n|  5|         6.5|          3|         5.8|        2.2|Iris-virginica|\n|  6|         7.6|          3|         6.6|        2.1|Iris-virginica|\n|  7|         4.9|        2.5|         4.5|        1.7|Iris-virginica|\n|  8|         7.3|        2.9|         6.3|        1.8|Iris-virginica|\n|  9|         6.7|        2.5|         5.8|        1.8|Iris-virginica|\n| 10|         7.2|        3.6|         6.1|        2.5|Iris-virginica|\n+---+------------+-----------+------------+-----------+--------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Distinct classes available in the Delta Table",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "%%sql\nSELECT distinct CLASS_NAME FROM deltalake_db.delta_table;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+\n|      CLASS_NAME|\n+----------------+\n|     Iris-setosa|\n|  Iris-virginica|\n| Iris-versicolor|\n|another-category|\n+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Perform Delete operation & inspect the changes in DeltaLake",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "%%sql # Read table from metastore\nDelete FROM deltalake_db.delta_table where CLASS_NAME='Iris-versicolor';",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|               51|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql # Read table from metastore\nselect * FROM deltalake_db.delta_table where CLASS_NAME='Iris-versicolor';",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+------------+-----------+------------+-----------+----------+\n| ID|SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH|CLASS_NAME|\n+---+------------+-----------+------------+-----------+----------+\n+---+------------+-----------+------------+-----------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql # Read table from metastore\nselect distinct class_name FROM deltalake_db.delta_table;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+\n|      class_name|\n+----------------+\n|     Iris-setosa|\n|  Iris-virginica|\n|another-category|\n+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Perform Update operation & inspect the changes in DeltaLake",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "%%sql # Read table from metastore\nUpdate  deltalake_db.delta_table set Class_Name='Testing update' where  \n((ID>20 and ID<30) or (ID>104 and ID<110)) and CLASS_NAME <>'another-category';",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|               14|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Read from Delta Lake table in spark sql",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"SELECT distinct class_name FROM deltalake_db.delta_table\"\"\" # Using a version number\nspark.sql(query).show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+\n|class_name      |\n+----------------+\n|another-category|\n|Iris-virginica  |\n|Testing update  |\n|Iris-setosa     |\n+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Read from Delta Lake table in using `delta` format",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "df = spark.read.format(\"delta\").load('s3://deltasparktesting/iris_data_delta/')\ndf.select('class_name').distinct().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+\n|      class_name|\n+----------------+\n|     Iris-setosa|\n|  Testing update|\n|  Iris-virginica|\n|another-category|\n+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Currently contributing files in Delta Table",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import *",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = spark.read.format(\"delta\").load('s3://deltasparktesting/iris_data_delta/')\ndf.select('CLASS_NAME', input_file_name()).distinct().show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+----------------------------------------------------------------------------------------------------------+\n|CLASS_NAME      |input_file_name()                                                                                         |\n+----------------+----------------------------------------------------------------------------------------------------------+\n|Iris-setosa     |s3://deltasparktesting/iris_data_delta/part-00001-63dfe2ff-af0f-43b8-8aaf-d267b1edace4-c000.snappy.parquet|\n|Testing update  |s3://deltasparktesting/iris_data_delta/part-00001-63dfe2ff-af0f-43b8-8aaf-d267b1edace4-c000.snappy.parquet|\n|another-category|s3://deltasparktesting/iris_data_delta/part-00002-cc1ae321-6d7b-466a-9454-a612f22874aa-c000.snappy.parquet|\n|Iris-virginica  |s3://deltasparktesting/iris_data_delta/part-00000-b1db4674-b819-4ee2-afa3-5fe7022375e0-c000.snappy.parquet|\n|Testing update  |s3://deltasparktesting/iris_data_delta/part-00000-b1db4674-b819-4ee2-afa3-5fe7022375e0-c000.snappy.parquet|\n+----------------+----------------------------------------------------------------------------------------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.columns #you can perform all generic spark transformation",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "['ID', 'SEPAL_LENGTH', 'SEPAL_WIDTH', 'PETAL_LENGTH', 'PETAL_WIDTH', 'CLASS_NAME']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# View History ",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"DESCRIBE HISTORY deltalake_db.delta_table \"\"\"\nspark.sql(query).show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+-------------------+------+--------+------------+--------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId|userName|operation   |operationParameters                                                                                                                         |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                  |userMetadata|engineInfo                                |\n+-------+-------------------+------+--------+------------+--------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|3      |2025-01-28 15:53:23|null  |null    |UPDATE      |{predicate -> ((((ID#2023 > 20) AND (ID#2023 < 30)) OR ((ID#2023 > 104) AND (ID#2023 < 110))) AND NOT (CLASS_NAME#2028 = another-category))}|null|null    |null     |2          |Serializable  |false        |{numRemovedFiles -> 2, numCopiedRows -> 63, numAddedChangeFiles -> 0, executionTimeMs -> 1633, scanTimeMs -> 1045, numAddedFiles -> 2, numUpdatedRows -> 14, rewriteTimeMs -> 584}|null        |Apache-Spark/3.3.0-amzn-1 Delta-Lake/2.1.0|\n|2      |2025-01-28 15:46:59|null  |null    |DELETE      |{predicate -> [\"(spark_catalog.deltalake_db.delta_table.CLASS_NAME = 'Iris-versicolor')\"]}                                                  |null|null    |null     |1          |Serializable  |false        |{numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 2305, numDeletedRows -> 51, scanTimeMs -> 1812, numAddedFiles -> 1, rewriteTimeMs -> 492} |null        |Apache-Spark/3.3.0-amzn-1 Delta-Lake/2.1.0|\n|1      |2025-01-28 15:39:50|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                                                                                         |null|null    |null     |0          |Serializable  |true         |{numFiles -> 4, numOutputRows -> 155, numOutputBytes -> 9495}                                                                                                                     |null        |Apache-Spark/3.3.0-amzn-1 Delta-Lake/2.1.0|\n|0      |2025-01-28 15:36:08|null  |null    |CREATE TABLE|{isManaged -> false, description -> null, partitionBy -> [], properties -> {}}                                                              |null|null    |null     |null       |Serializable  |true         |{}                                                                                                                                                                                |null        |Apache-Spark/3.3.0-amzn-1 Delta-Lake/2.1.0|\n+-------+-------------------+------+--------+------------+--------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"SELECT distinct class_name FROM deltalake_db.delta_table\"\"\"\nspark.sql(query).show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+\n|class_name      |\n+----------------+\n|Iris-setosa     |\n|Testing update  |\n|Iris-virginica  |\n|another-category|\n+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Perform Time Travel",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "df_previous = spark.read.format(\"delta\").option(\"versionAsof\", 1).load('s3://deltasparktesting/iris_data_delta/')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_previous.select('class_name').distinct().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+\n|      class_name|\n+----------------+\n|     Iris-setosa|\n|another-category|\n| Iris-versicolor|\n|  Iris-virginica|\n+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_previous_2 = spark.read.format(\"delta\").option(\"versionAsof\", 0).load('s3://deltasparktesting/iris_data_delta/')\ndf_previous_2.select('class_name').distinct().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+\n|class_name|\n+----------+\n+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# Another approach to read Delta Table from Catalog",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "df_products_read = spark.table(\"deltalake_db.delta_table\")\ndf_products_read.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: 0b90dd84-fb05-47e6-849b-a921fe1a78e9\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session 0b90dd84-fb05-47e6-849b-a921fe1a78e9 to get into ready status...\nSession 0b90dd84-fb05-47e6-849b-a921fe1a78e9 has been created.\n+---+------------+-----------+------------+-----------+--------------+\n| ID|SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH|    CLASS_NAME|\n+---+------------+-----------+------------+-----------+--------------+\n|  1|         6.3|        3.3|           6|        2.5|Iris-virginica|\n|  2|         5.8|        2.7|         5.1|        1.9|Iris-virginica|\n|  3|         7.1|          3|         5.9|        2.1|Iris-virginica|\n|  4|         6.3|        2.9|         5.6|        1.8|Iris-virginica|\n|  5|         6.5|          3|         5.8|        2.2|Iris-virginica|\n|  6|         7.6|          3|         6.6|        2.1|Iris-virginica|\n|  7|         4.9|        2.5|         4.5|        1.7|Iris-virginica|\n|  8|         7.3|        2.9|         6.3|        1.8|Iris-virginica|\n|  9|         6.7|        2.5|         5.8|        1.8|Iris-virginica|\n| 10|         7.2|        3.6|         6.1|        2.5|Iris-virginica|\n| 11|         6.5|        3.2|         5.1|          2|Iris-virginica|\n| 12|         6.4|        2.7|         5.3|        1.9|Iris-virginica|\n| 13|         6.8|          3|         5.5|        2.1|Iris-virginica|\n| 14|         5.7|        2.5|           5|          2|Iris-virginica|\n| 15|         5.8|        2.8|         5.1|        2.4|Iris-virginica|\n| 16|         6.4|        3.2|         5.3|        2.3|Iris-virginica|\n| 17|         6.5|          3|         5.5|        1.8|Iris-virginica|\n| 18|         7.7|        3.8|         6.7|        2.2|Iris-virginica|\n| 19|         7.7|        2.6|         6.9|        2.3|Iris-virginica|\n| 20|           6|        2.2|           5|        1.5|Iris-virginica|\n+---+------------+-----------+------------+-----------+--------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import *",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_products_read.select('CLASS_NAME', input_file_name()).distinct().show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+----------------------------------------------------------------------------------------------------------+\n|CLASS_NAME      |input_file_name()                                                                                         |\n+----------------+----------------------------------------------------------------------------------------------------------+\n|Iris-virginica  |s3://deltasparktesting/iris_data_delta/part-00000-b1db4674-b819-4ee2-afa3-5fe7022375e0-c000.snappy.parquet|\n|Testing update  |s3://deltasparktesting/iris_data_delta/part-00000-b1db4674-b819-4ee2-afa3-5fe7022375e0-c000.snappy.parquet|\n|another-category|s3://deltasparktesting/iris_data_delta/part-00002-cc1ae321-6d7b-466a-9454-a612f22874aa-c000.snappy.parquet|\n|Iris-setosa     |s3://deltasparktesting/iris_data_delta/part-00001-63dfe2ff-af0f-43b8-8aaf-d267b1edace4-c000.snappy.parquet|\n|Testing update  |s3://deltasparktesting/iris_data_delta/part-00001-63dfe2ff-af0f-43b8-8aaf-d267b1edace4-c000.snappy.parquet|\n+----------------+----------------------------------------------------------------------------------------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Use the Delta table path instead of the table name in the query. This directly reads the data from the path without checking the table statistics.",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"SELECT * FROM delta.`s3://deltasparktesting/iris_data_delta/`\"\"\"\ndf_path=spark.sql(query)\ndf_path.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+------------+-----------+------------+-----------+--------------+\n| ID|SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH|    CLASS_NAME|\n+---+------------+-----------+------------+-----------+--------------+\n|  1|         6.3|        3.3|           6|        2.5|Iris-virginica|\n|  2|         5.8|        2.7|         5.1|        1.9|Iris-virginica|\n|  3|         7.1|          3|         5.9|        2.1|Iris-virginica|\n|  4|         6.3|        2.9|         5.6|        1.8|Iris-virginica|\n|  5|         6.5|          3|         5.8|        2.2|Iris-virginica|\n|  6|         7.6|          3|         6.6|        2.1|Iris-virginica|\n|  7|         4.9|        2.5|         4.5|        1.7|Iris-virginica|\n|  8|         7.3|        2.9|         6.3|        1.8|Iris-virginica|\n|  9|         6.7|        2.5|         5.8|        1.8|Iris-virginica|\n| 10|         7.2|        3.6|         6.1|        2.5|Iris-virginica|\n| 11|         6.5|        3.2|         5.1|          2|Iris-virginica|\n| 12|         6.4|        2.7|         5.3|        1.9|Iris-virginica|\n| 13|         6.8|          3|         5.5|        2.1|Iris-virginica|\n| 14|         5.7|        2.5|           5|          2|Iris-virginica|\n| 15|         5.8|        2.8|         5.1|        2.4|Iris-virginica|\n| 16|         6.4|        3.2|         5.3|        2.3|Iris-virginica|\n| 17|         6.5|          3|         5.5|        1.8|Iris-virginica|\n| 18|         7.7|        3.8|         6.7|        2.2|Iris-virginica|\n| 19|         7.7|        2.6|         6.9|        2.3|Iris-virginica|\n| 20|           6|        2.2|           5|        1.5|Iris-virginica|\n+---+------------+-----------+------------+-----------+--------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df2=df_path.select('CLASS_NAME', input_file_name().alias(\"input_file_name\")).distinct()\ndf2.show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+----------------------------------------------------------------------------------------------------------+\n|CLASS_NAME      |input_file_name                                                                                           |\n+----------------+----------------------------------------------------------------------------------------------------------+\n|Iris-setosa     |s3://deltasparktesting/iris_data_delta/part-00001-63dfe2ff-af0f-43b8-8aaf-d267b1edace4-c000.snappy.parquet|\n|Testing update  |s3://deltasparktesting/iris_data_delta/part-00001-63dfe2ff-af0f-43b8-8aaf-d267b1edace4-c000.snappy.parquet|\n|Iris-virginica  |s3://deltasparktesting/iris_data_delta/part-00000-b1db4674-b819-4ee2-afa3-5fe7022375e0-c000.snappy.parquet|\n|Testing update  |s3://deltasparktesting/iris_data_delta/part-00000-b1db4674-b819-4ee2-afa3-5fe7022375e0-c000.snappy.parquet|\n|another-category|s3://deltasparktesting/iris_data_delta/part-00002-cc1ae321-6d7b-466a-9454-a612f22874aa-c000.snappy.parquet|\n+----------------+----------------------------------------------------------------------------------------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Writing Data in DeltaLake",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Create table in the metastore using DataFrame's schema and write data to it\ndf2.write.format(\"delta\").mode(\"overwrite\").option(\"path\",'s3://deltasparktesting/iris_data_delta_2/') \\\n.saveAsTable(\"deltalake_db.delta_table_2\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df3=df2.filter(df2[\"CLASS_NAME\"].isin(\"Iris-setosa\",\"Iris-virginica\"))\ndf3.show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------+----------------------------------------------------------------------------------------------------------+\n|CLASS_NAME    |input_file_name                                                                                           |\n+--------------+----------------------------------------------------------------------------------------------------------+\n|Iris-setosa   |s3://deltasparktesting/iris_data_delta/part-00001-63dfe2ff-af0f-43b8-8aaf-d267b1edace4-c000.snappy.parquet|\n|Iris-virginica|s3://deltasparktesting/iris_data_delta/part-00000-b1db4674-b819-4ee2-afa3-5fe7022375e0-c000.snappy.parquet|\n+--------------+----------------------------------------------------------------------------------------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df3.write.format(\"delta\").mode(\"overwrite\").save('s3://deltasparktesting/iris_data_delta_2/')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Read from Delta table via DeltaLake library",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "Learn more about this module here: https://docs.delta.io/latest/quick-start.html",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "from delta.tables import *",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Query table from metastore",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "deltaTable = DeltaTable.forName(spark, f\"deltalake_db.delta_table\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "deltaTable.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "AttributeError: 'DeltaTable' object has no attribute 'show'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "deltaTable.toDF().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+------------+-----------+------------+-----------+--------------+\n| ID|SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH|    CLASS_NAME|\n+---+------------+-----------+------------+-----------+--------------+\n|  1|         6.3|        3.3|           6|        2.5|Iris-virginica|\n|  2|         5.8|        2.7|         5.1|        1.9|Iris-virginica|\n|  3|         7.1|          3|         5.9|        2.1|Iris-virginica|\n|  4|         6.3|        2.9|         5.6|        1.8|Iris-virginica|\n|  5|         6.5|          3|         5.8|        2.2|Iris-virginica|\n|  6|         7.6|          3|         6.6|        2.1|Iris-virginica|\n|  7|         4.9|        2.5|         4.5|        1.7|Iris-virginica|\n|  8|         7.3|        2.9|         6.3|        1.8|Iris-virginica|\n|  9|         6.7|        2.5|         5.8|        1.8|Iris-virginica|\n| 10|         7.2|        3.6|         6.1|        2.5|Iris-virginica|\n| 11|         6.5|        3.2|         5.1|          2|Iris-virginica|\n| 12|         6.4|        2.7|         5.3|        1.9|Iris-virginica|\n| 13|         6.8|          3|         5.5|        2.1|Iris-virginica|\n| 14|         5.7|        2.5|           5|          2|Iris-virginica|\n| 15|         5.8|        2.8|         5.1|        2.4|Iris-virginica|\n| 16|         6.4|        3.2|         5.3|        2.3|Iris-virginica|\n| 17|         6.5|          3|         5.5|        1.8|Iris-virginica|\n| 18|         7.7|        3.8|         6.7|        2.2|Iris-virginica|\n| 19|         7.7|        2.6|         6.9|        2.3|Iris-virginica|\n| 20|           6|        2.2|           5|        1.5|Iris-virginica|\n+---+------------+-----------+------------+-----------+--------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df=deltaTable.toDF()\ndf.select(\"class_name\").distinct().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+\n|      class_name|\n+----------------+\n|     Iris-setosa|\n|  Testing update|\n|  Iris-virginica|\n|another-category|\n+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Query table from s3 path",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "deltaTable_2 = DeltaTable.forPath(spark, 's3://deltasparktesting/iris_data_delta/') #query table from path\ndeltaTable_2.toDF().select(\"class_name\").distinct().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+\n|      class_name|\n+----------------+\n|     Iris-setosa|\n|  Testing update|\n|  Iris-virginica|\n|another-category|\n+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Delete operation with Delta-Spark",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Declare the predicate by using a SQL-formatted string.\ndeltaTable_2.delete(\"id = '100'\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Update operation with Delta-Spark",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "df5=deltaTable_2.toDF()\ndf5.filter(df5[\"id\"]=='119').show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+------------+-----------+------------+-----------+-----------+\n|ID |SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH|CLASS_NAME |\n+---+------------+-----------+------------+-----------+-----------+\n|119|5.1         |3.8        |1.5         |0.3        |Iris-setosa|\n+---+------------+-----------+------------+-----------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "deltaTable_2.update(\n    condition = \"id = '119'\",\n    set = { \"PETAL_LENGTH\": \"'1000'\" }\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df5=deltaTable_2.toDF()\ndf5.filter(df5[\"id\"]=='119').show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+------------+-----------+------------+-----------+-----------+\n|ID |SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH|CLASS_NAME |\n+---+------------+-----------+------------+-----------+-----------+\n|119|5.1         |3.8        |1000        |0.3        |Iris-setosa|\n+---+------------+-----------+------------+-----------+-----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Merge operation with Delta-Spark",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nimport time\n\nut = time.time()\n\nproduct = [\n    {'product_id': '00001', 'product_name': 'Heater', 'price': 250, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00002', 'product_name': 'Thermostat', 'price': 400, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00003', 'product_name': 'Television', 'price': 600, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00004', 'product_name': 'Blender', 'price': 100, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00005', 'product_name': 'USB charger', 'price': 50, 'category': 'Electronics', 'updated_at': ut}\n]\n\ndf_products = spark.createDataFrame(Row(**x) for x in product)\ndf_products.show()\ndf_products.write.format(\"delta\").mode(\"overwrite\").option(\"path\",\"s3://deltasparktesting/iris_data_delta_3/\").saveAsTable(f\"deltalake_db.delta_table_3\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: 61d2521e-3acc-477f-9005-0cf18f05d456\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session 61d2521e-3acc-477f-9005-0cf18f05d456 to get into ready status...\nSession 61d2521e-3acc-477f-9005-0cf18f05d456 has been created.\n+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00001|      Heater|  250|Electronics|1.7384802013919556E9|\n|     00002|  Thermostat|  400|Electronics|1.7384802013919556E9|\n|     00003|  Television|  600|Electronics|1.7384802013919556E9|\n|     00004|     Blender|  100|Electronics|1.7384802013919556E9|\n|     00005| USB charger|   50|Electronics|1.7384802013919556E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "ut = time.time()\n\nproduct_updates = [\n    {'product_id': '00001', 'product_name': 'Heater', 'price': 400, 'category': 'Electronics', 'updated_at': ut}, # Update\n    {'product_id': '00007', 'product_name': 'Chair', 'price': 50, 'category': 'Furniture','updated_at': ut} # Insert\n]\ndf_product_updates = spark.createDataFrame(Row(**x) for x in product_updates)\ndf_product_updates.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00001|      Heater|  400|Electronics|1.7384806456987453E9|\n|     00007|       Chair|   50|  Furniture|1.7384806456987453E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "deltaTable = DeltaTable.forName(spark, \"deltalake_db.delta_table_3\")\ndeltaTable.toDF().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00005| USB charger|   50|Electronics|1.7384802013919556E9|\n|     00002|  Thermostat|  400|Electronics|1.7384802013919556E9|\n|     00003|  Television|  600|Electronics|1.7384802013919556E9|\n|     00004|     Blender|  100|Electronics|1.7384802013919556E9|\n|     00001|      Heater|  250|Electronics|1.7384802013919556E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Sample SQL Merge statement\nMERGE INTO target_table\nUSING source_table\nON merge_condition\nWHEN MATCHED THEN\n   UPDATE SET column1 = value1 [, column2 = value2 …]\nWHEN NOT MATCHED THEN\n   INSERT (column1 [, column2 …])\n   VALUES (value1 [, value2 …]);\n ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "deltaTable.alias(\"products\").merge(\n    df_product_updates.alias(\"updates\"),\n    \"products.product_id = updates.product_id\") \\\n    .whenMatchedUpdate(set = {\n        \"product_name\": \"updates.product_name\",\n        \"price\": \"updates.price\",\n        \"category\": \"updates.category\",\n        \"updated_at\": \"updates.updated_at\"    } ) \\\n    .whenNotMatchedInsert(values = {\n        \"product_id\": \"updates.product_id\",\n        \"product_name\": \"updates.product_name\",\n        \"price\": \"updates.price\",\n        \"category\": \"updates.category\",\n        \"updated_at\": \"updates.updated_at\"}\n) \\\n.execute()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "deltaTable.toDF().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00005| USB charger|   50|Electronics|1.7384802013919556E9|\n|     00002|  Thermostat|  400|Electronics|1.7384802013919556E9|\n|     00003|  Television|  600|Electronics|1.7384802013919556E9|\n|     00001|      Heater|  400|Electronics|1.7384806456987453E9|\n|     00007|       Chair|   50|  Furniture|1.7384806456987453E9|\n|     00004|     Blender|  100|Electronics|1.7384802013919556E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Optimizing Delta Lake tables",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nimport time\n\nut = time.time()\n\nproduct = [\n    {'product_id': '00001', 'product_name': 'Heater', 'price': 250, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00002', 'product_name': 'Thermostat', 'price': 400, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00003', 'product_name': 'Television', 'price': 600, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00004', 'product_name': 'Blender', 'price': 100, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00005', 'product_name': 'USB charger', 'price': 50, 'category': 'Electronics', 'updated_at': ut}\n]\n\ndf_products = spark.createDataFrame(Row(**x) for x in product)\ndf_products.show()\ndf_products.write.format(\"delta\").mode(\"overwrite\").option(\"path\",\"s3://deltasparktesting/iris_data_delta_4/\").saveAsTable(f\"deltalake_db.delta_table_4\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: 136215b1-bd14-4f18-8eeb-a90dcdddf5e4\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session 136215b1-bd14-4f18-8eeb-a90dcdddf5e4 to get into ready status...\nSession 136215b1-bd14-4f18-8eeb-a90dcdddf5e4 has been created.\n+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00001|      Heater|  250|Electronics|1.7389311233048344E9|\n|     00002|  Thermostat|  400|Electronics|1.7389311233048344E9|\n|     00003|  Television|  600|Electronics|1.7389311233048344E9|\n|     00004|     Blender|  100|Electronics|1.7389311233048344E9|\n|     00005| USB charger|   50|Electronics|1.7389311233048344E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from delta.tables import *\ndeltaTable = DeltaTable.forPath(spark, \"s3://deltasparktesting/iris_data_delta_4/\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "deltaTable.optimize().executeCompaction()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>>]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nOPTIMIZE delta.`s3://deltasparktesting/iris_data_delta_4/`",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+--------------------+\n|                path|             metrics|\n+--------------------+--------------------+\n|s3://deltasparkte...|{0, 0, {null, nul...|\n+--------------------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "If you run OPTIMIZE twice on the same dataset, the second run will have no effect. This is also known as an idempotent operation.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%%sql\nOPTIMIZE delta.`s3://deltasparktesting/iris_data_delta_4/`",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+--------------------+\n|                path|             metrics|\n+--------------------+--------------------+\n|s3://deltasparkte...|{0, 0, {null, nul...|\n+--------------------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "It merges smaller files into bigger ones to enhance storage efficiency and performance. This means that if you read the data before and after running OPTIMIZE, you will get the same results.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df=spark.read.format('delta').load('s3://deltasparktesting/iris_data_delta_4/')\ndf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00005| USB charger|   50|Electronics|1.7389311233048344E9|\n|     00002|  Thermostat|  400|Electronics|1.7389311233048344E9|\n|     00003|  Television|  600|Electronics|1.7389311233048344E9|\n|     00004|     Blender|  100|Electronics|1.7389311233048344E9|\n|     00001|      Heater|  250|Electronics|1.7389311233048344E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "OPTIMIZE is a Delta utility function that comes in two variants: Z-Order and bin-packing. The default is bin-packing.\n\nWhat exactly is bin-packing? At a high level, this is a technique that is used to coalesce many small files into fewer large files across an arbitrary number of bins. A bin is defined as a file of a maximum file size (the default for Spark Delta Lake is 1 GB).",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "query = f\"\"\"DESCRIBE HISTORY deltalake_db.delta_table_4 \"\"\"\nspark.sql(query).show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+-------------------+------+--------+---------------------------------+------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId|userName|operation                        |operationParameters                                                           |job |notebook|clusterId|readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                   |userMetadata|engineInfo                                |\n+-------+-------------------+------+--------+---------------------------------+------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|1      |2025-02-07 12:31:51|null  |null    |OPTIMIZE                         |{predicate -> [], zOrderBy -> []}                                             |null|null    |null     |0          |SnapshotIsolation|false        |{numRemovedFiles -> 6, numRemovedBytes -> 8514, p25FileSize -> 1717, minFileSize -> 1717, numAddedFiles -> 1, maxFileSize -> 1717, p75FileSize -> 1717, p50FileSize -> 1717, numAddedBytes -> 1717}|null        |Apache-Spark/3.3.0-amzn-1 Delta-Lake/2.1.0|\n|0      |2025-02-07 12:26:13|null  |null    |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> false, description -> null, partitionBy -> [], properties -> {}}|null|null    |null     |null       |Serializable     |false        |{numFiles -> 6, numOutputRows -> 5, numOutputBytes -> 8514}                                                                                                                                        |null        |Apache-Spark/3.3.0-amzn-1 Delta-Lake/2.1.0|\n+-------+-------------------+------+--------+---------------------------------+------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col\n(DeltaTable.forName(spark, \"deltalake_db.delta_table_4\")\n    .history(10)\n    .where(col(\"operation\") == \"OPTIMIZE\")\n    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics.numRemovedFiles\", \"operationMetrics.numAddedFiles\")\n    .show(truncate=False))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+-------------------+---------+---------------+-------------+\n|version|timestamp          |operation|numRemovedFiles|numAddedFiles|\n+-------+-------------------+---------+---------------+-------------+\n|1      |2025-02-07 12:31:51|OPTIMIZE |6              |1            |\n+-------+-------------------+---------+---------------+-------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Schema Evolution",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "Delta Lake utilizes the technique from transactional Data Warehouses called schema-on-write. This simply means that, there is a process in place to check the schema of the write against the table prior to a write operation being executed. ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 0c7ad097-0a9b-4890-bd20-9ef0a296d7bc\nStopped session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nfrom pyspark.sql.functions import *\nimport time\n\nut = time.time()\n\nproduct = [\n    {'product_id': '00001', 'product_name': 'Heater', 'price': 250, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00002', 'product_name': 'Thermostat', 'price': 400, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00003', 'product_name': 'Television', 'price': 600, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00004', 'product_name': 'Blender', 'price': 100, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00005', 'product_name': 'USB charger', 'price': 50, 'category': 'Electronics', 'updated_at': ut}\n]\n\ndf_products = spark.createDataFrame(Row(**x) for x in product)\ndf_products.show()\ndf_products.write.format(\"delta\").mode(\"overwrite\").option(\"path\",\"s3://deltasparktesting/iris_data_delta_5/\").saveAsTable(f\"deltalake_db.delta_table_5\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: native-delta-sql--dde9c6d8-d7f9-46ea-ab95-3c97b064f3d9\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session native-delta-sql--dde9c6d8-d7f9-46ea-ab95-3c97b064f3d9 to get into ready status...\nSession native-delta-sql--dde9c6d8-d7f9-46ea-ab95-3c97b064f3d9 has been created.\n+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00001|      Heater|  250|Electronics|1.7390275267700424E9|\n|     00002|  Thermostat|  400|Electronics|1.7390275267700424E9|\n|     00003|  Television|  600|Electronics|1.7390275267700424E9|\n|     00004|     Blender|  100|Electronics|1.7390275267700424E9|\n|     00005| USB charger|   50|Electronics|1.7390275267700424E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df=spark.read.format('delta').load('s3://deltasparktesting/iris_data_delta_5/')\ndf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00005| USB charger|   50|Electronics|1.7390275267700424E9|\n|     00002|  Thermostat|  400|Electronics|1.7390275267700424E9|\n|     00003|  Television|  600|Electronics|1.7390275267700424E9|\n|     00004|     Blender|  100|Electronics|1.7390275267700424E9|\n|     00001|      Heater|  250|Electronics|1.7390275267700424E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df2=df.withColumn('one_dummy_column',lit(5))\ndf2.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+----------------+\n|product_id|product_name|price|   category|          updated_at|one_dummy_column|\n+----------+------------+-----+-----------+--------------------+----------------+\n|     00005| USB charger|   50|Electronics|1.7390275267700424E9|               5|\n|     00002|  Thermostat|  400|Electronics|1.7390275267700424E9|               5|\n|     00003|  Television|  600|Electronics|1.7390275267700424E9|               5|\n|     00004|     Blender|  100|Electronics|1.7390275267700424E9|               5|\n|     00001|      Heater|  250|Electronics|1.7390275267700424E9|               5|\n+----------+------------+-----+-----------+--------------------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df2.write.format(\"delta\").mode(\"append\").option(\"path\",\"s3://deltasparktesting/iris_data_delta_5/\").saveAsTable(f\"deltalake_db.delta_table_5\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: ec4bbf00-e9bc-43b4-b0d0-31865c183802).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- product_id: string (nullable = true)\n-- product_name: string (nullable = true)\n-- price: long (nullable = true)\n-- category: string (nullable = true)\n-- updated_at: double (nullable = true)\n\n\nData schema:\nroot\n-- product_id: string (nullable = true)\n-- product_name: string (nullable = true)\n-- price: long (nullable = true)\n-- category: string (nullable = true)\n-- updated_at: double (nullable = true)\n-- one_dummy_column: integer (nullable = true)\n\n         \n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df2.write.format(\"delta\").mode(\"append\").option('mergeSchema',True).option(\"path\",\"s3://deltasparktesting/iris_data_delta_5/\").saveAsTable(f\"deltalake_db.delta_table_5\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Alternative to automatic Schema Evolution",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "When we use .option('mergeSchema',True) to modify the behavior of the Delta Lake write,it comes at the price of our not being fully aware of the changes to our table schema.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df=spark.read.format('delta').load('s3://deltasparktesting/iris_data_delta_5/')\ndf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+----------------+\n|product_id|product_name|price|   category|          updated_at|one_dummy_column|\n+----------+------------+-----+-----------+--------------------+----------------+\n|     00005| USB charger|   50|Electronics|1.7390275267700424E9|               5|\n|     00002|  Thermostat|  400|Electronics|1.7390275267700424E9|               5|\n|     00003|  Television|  600|Electronics|1.7390275267700424E9|               5|\n|     00004|     Blender|  100|Electronics|1.7390275267700424E9|               5|\n|     00001|      Heater|  250|Electronics|1.7390275267700424E9|               5|\n|     00005| USB charger|   50|Electronics|1.7390275267700424E9|            null|\n|     00002|  Thermostat|  400|Electronics|1.7390275267700424E9|            null|\n|     00003|  Television|  600|Electronics|1.7390275267700424E9|            null|\n|     00004|     Blender|  100|Electronics|1.7390275267700424E9|            null|\n|     00001|      Heater|  250|Electronics|1.7390275267700424E9|            null|\n+----------+------------+-----+-----------+--------------------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df2=df.withColumn('another_dummy_column',lit(10))\ndf2.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+----------------+--------------------+\n|product_id|product_name|price|   category|          updated_at|one_dummy_column|another_dummy_column|\n+----------+------------+-----+-----------+--------------------+----------------+--------------------+\n|     00005| USB charger|   50|Electronics|1.7390275267700424E9|               5|                  10|\n|     00002|  Thermostat|  400|Electronics|1.7390275267700424E9|               5|                  10|\n|     00003|  Television|  600|Electronics|1.7390275267700424E9|               5|                  10|\n|     00004|     Blender|  100|Electronics|1.7390275267700424E9|               5|                  10|\n|     00001|      Heater|  250|Electronics|1.7390275267700424E9|               5|                  10|\n|     00005| USB charger|   50|Electronics|1.7390275267700424E9|            null|                  10|\n|     00002|  Thermostat|  400|Electronics|1.7390275267700424E9|            null|                  10|\n|     00003|  Television|  600|Electronics|1.7390275267700424E9|            null|                  10|\n|     00004|     Blender|  100|Electronics|1.7390275267700424E9|            null|                  10|\n|     00001|      Heater|  250|Electronics|1.7390275267700424E9|            null|                  10|\n+----------+------------+-----+-----------+--------------------+----------------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df2.write.format(\"delta\").mode(\"append\").option(\"path\",\"s3://deltasparktesting/iris_data_delta_5/\").saveAsTable(f\"deltalake_db.delta_table_5\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: ec4bbf00-e9bc-43b4-b0d0-31865c183802).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- product_id: string (nullable = true)\n-- product_name: string (nullable = true)\n-- price: long (nullable = true)\n-- category: string (nullable = true)\n-- updated_at: double (nullable = true)\n-- one_dummy_column: integer (nullable = true)\n\n\nData schema:\nroot\n-- product_id: string (nullable = true)\n-- product_name: string (nullable = true)\n-- price: long (nullable = true)\n-- category: string (nullable = true)\n-- updated_at: double (nullable = true)\n-- one_dummy_column: integer (nullable = true)\n-- another_dummy_column: integer (nullable = true)\n\n         \n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql('Alter table deltalake_db.delta_table_5 add columns (another_dummy_column Int);')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df2.write.format(\"delta\").mode(\"append\").option(\"path\",\"s3://deltasparktesting/iris_data_delta_5/\").saveAsTable(f\"deltalake_db.delta_table_5\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Restoring Your Table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from delta.tables import *\ndt = DeltaTable.forPath(spark, \"s3://deltasparktesting/iris_data_delta_5/\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: a11cda89-2578-42f8-b492-a199d04ad8fd\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session a11cda89-2578-42f8-b492-a199d04ad8fd to get into ready status...\nSession a11cda89-2578-42f8-b492-a199d04ad8fd has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dt.history().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n|      3|2025-02-08 15:22:23|  null|    null|               WRITE|{mode -> Append, ...|null|    null|     null|          2|  Serializable|        false|{numFiles -> 10, ...|        null|Apache-Spark/3.3....|\n|      2|2025-02-08 15:21:54|  null|    null|         ADD COLUMNS|{columns -> [{\"co...|null|    null|     null|          1|  Serializable|         true|                  {}|        null|Apache-Spark/3.3....|\n|      1|2025-02-08 15:16:15|  null|    null|               WRITE|{mode -> Append, ...|null|    null|     null|          0|  Serializable|        false|{numFiles -> 5, n...|        null|Apache-Spark/3.3....|\n|      0|2025-02-08 15:13:06|  null|    null|CREATE OR REPLACE...|{isManaged -> fal...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dt.restoreToVersion(0)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Parquet to Delta Conversion",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nfrom pyspark.sql.functions import *\nimport time\n\nut = time.time()\n\nproduct = [\n    {'product_id': '00001', 'product_name': 'Heater', 'price': 250, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00002', 'product_name': 'Thermostat', 'price': 400, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00003', 'product_name': 'Television', 'price': 600, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00004', 'product_name': 'Blender', 'price': 100, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00005', 'product_name': 'USB charger', 'price': 50, 'category': 'Electronics', 'updated_at': ut}\n]\n\ndf_products = spark.createDataFrame(Row(**x) for x in product)\ndf_products.show()\ndf_products.write.format(\"parquet\").mode(\"overwrite\").option(\"path\",\"s3://deltasparktesting/iris_data_delta_7/\").saveAsTable(f\"deltalake_db.delta_table_7\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nSession ID: 4de62d3a-542f-4a2f-b348-d802d5d8a2da\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session 4de62d3a-542f-4a2f-b348-d802d5d8a2da to get into ready status...\nSession 4de62d3a-542f-4a2f-b348-d802d5d8a2da has been created.\n+----------+------------+-----+-----------+-------------------+\n|product_id|product_name|price|   category|         updated_at|\n+----------+------------+-----+-----------+-------------------+\n|     00001|      Heater|  250|Electronics|1.739265821850448E9|\n|     00002|  Thermostat|  400|Electronics|1.739265821850448E9|\n|     00003|  Television|  600|Electronics|1.739265821850448E9|\n|     00004|     Blender|  100|Electronics|1.739265821850448E9|\n|     00005| USB charger|   50|Electronics|1.739265821850448E9|\n+----------+------------+-----+-----------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Since a Delta Lake table composed of Prquet file internally, the transaction log is the biggest difference when converting a Parquet table to a Delta Lake.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%%sql\nconvert to delta deltalake_db.delta_table_7;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: 54d65640-6ad3-4b22-b337-78a7a2adf5dc\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session 54d65640-6ad3-4b22-b337-78a7a2adf5dc to get into ready status...\nSession 54d65640-6ad3-4b22-b337-78a7a2adf5dc has been created.\n++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Convert from Parquet to Delta using s3 path",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nfrom pyspark.sql.functions import *\nimport time\n\nut = time.time()\n\nproduct = [\n    {'product_id': '00001', 'product_name': 'Heater', 'price': 250, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00002', 'product_name': 'Thermostat', 'price': 400, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00003', 'product_name': 'Television', 'price': 600, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00004', 'product_name': 'Blender', 'price': 100, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00005', 'product_name': 'USB charger', 'price': 50, 'category': 'Electronics', 'updated_at': ut}\n]\n\ndf_products = spark.createDataFrame(Row(**x) for x in product)\ndf_products.show()\ndf_products.write.format(\"parquet\").mode(\"overwrite\").option(\"path\",\"s3://deltasparktesting/iris_data_delta_8/\").saveAsTable(f\"deltalake_db.delta_table_8\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00001|      Heater|  250|Electronics|1.7392662504966307E9|\n|     00002|  Thermostat|  400|Electronics|1.7392662504966307E9|\n|     00003|  Television|  600|Electronics|1.7392662504966307E9|\n|     00004|     Blender|  100|Electronics|1.7392662504966307E9|\n|     00005| USB charger|   50|Electronics|1.7392662504966307E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nCONVERT TO DELTA parquet.`s3://deltasparktesting/iris_data_delta_8/`",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Converting partitioned parquet table to Delta",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nfrom pyspark.sql.functions import *\nimport time\n\ndf=spark.read.format('csv').option('header',True).option('inferSchema',True).load('s3://deltasparktesting/iris_data/');\ndf.show()\ndf.write.format(\"parquet\").mode(\"overwrite\").partitionBy('CLASS_NAME').option(\"path\",\"s3://deltasparktesting/iris_data_delta_9/\").saveAsTable(f\"deltalake_db.delta_table_9\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+------------+-----------+------------+-----------+---------------+\n| Id|SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH|     CLASS_NAME|\n+---+------------+-----------+------------+-----------+---------------+\n|  1|         7.0|        3.2|         4.7|        1.4|Iris-versicolor|\n|  2|         6.4|        3.2|         4.5|        1.5|Iris-versicolor|\n|  3|         6.9|        3.1|         4.9|        1.5|Iris-versicolor|\n|  4|         5.5|        2.3|         4.0|        1.3|Iris-versicolor|\n|  5|         6.5|        2.8|         4.6|        1.5|Iris-versicolor|\n|  6|         5.7|        2.8|         4.5|        1.3|Iris-versicolor|\n|  7|         6.3|        3.3|         4.7|        1.6|Iris-versicolor|\n|  8|         4.9|        2.4|         3.3|        1.0|Iris-versicolor|\n|  9|         6.6|        2.9|         4.6|        1.3|Iris-versicolor|\n| 10|         5.2|        2.7|         3.9|        1.4|Iris-versicolor|\n| 11|         5.0|        2.0|         3.5|        1.0|Iris-versicolor|\n| 12|         5.9|        3.0|         4.0|        1.5|Iris-versicolor|\n| 13|         6.0|        2.2|         4.0|        1.0|Iris-versicolor|\n| 14|         6.1|        2.9|         4.7|        1.4|Iris-versicolor|\n| 15|         5.6|        2.9|         3.6|        1.3|Iris-versicolor|\n| 16|         6.7|        3.1|         4.4|        1.4|Iris-versicolor|\n| 17|         5.6|        3.0|         4.5|        1.5|Iris-versicolor|\n| 18|         5.8|        2.7|         4.1|        1.0|Iris-versicolor|\n| 19|         6.2|        2.2|         4.5|        1.5|Iris-versicolor|\n| 20|         5.6|        2.5|         3.9|        1.1|Iris-versicolor|\n+---+------------+-----------+------------+-----------+---------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nconvert to delta deltalake_db.delta_table_9;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nfrom pyspark.sql.functions import *\nimport time\n\ndf=spark.read.format('csv').option('header',True).option('inferSchema',True).load('s3://deltasparktesting/iris_data/');\ndf.show()\ndf.write.format(\"parquet\").mode(\"overwrite\").partitionBy('CLASS_NAME').option(\"path\",\"s3://deltasparktesting/iris_data_delta_10/\").saveAsTable(f\"deltalake_db.delta_table_10\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+------------+-----------+------------+-----------+---------------+\n| Id|SEPAL_LENGTH|SEPAL_WIDTH|PETAL_LENGTH|PETAL_WIDTH|     CLASS_NAME|\n+---+------------+-----------+------------+-----------+---------------+\n|  1|         7.0|        3.2|         4.7|        1.4|Iris-versicolor|\n|  2|         6.4|        3.2|         4.5|        1.5|Iris-versicolor|\n|  3|         6.9|        3.1|         4.9|        1.5|Iris-versicolor|\n|  4|         5.5|        2.3|         4.0|        1.3|Iris-versicolor|\n|  5|         6.5|        2.8|         4.6|        1.5|Iris-versicolor|\n|  6|         5.7|        2.8|         4.5|        1.3|Iris-versicolor|\n|  7|         6.3|        3.3|         4.7|        1.6|Iris-versicolor|\n|  8|         4.9|        2.4|         3.3|        1.0|Iris-versicolor|\n|  9|         6.6|        2.9|         4.6|        1.3|Iris-versicolor|\n| 10|         5.2|        2.7|         3.9|        1.4|Iris-versicolor|\n| 11|         5.0|        2.0|         3.5|        1.0|Iris-versicolor|\n| 12|         5.9|        3.0|         4.0|        1.5|Iris-versicolor|\n| 13|         6.0|        2.2|         4.0|        1.0|Iris-versicolor|\n| 14|         6.1|        2.9|         4.7|        1.4|Iris-versicolor|\n| 15|         5.6|        2.9|         3.6|        1.3|Iris-versicolor|\n| 16|         6.7|        3.1|         4.4|        1.4|Iris-versicolor|\n| 17|         5.6|        3.0|         4.5|        1.5|Iris-versicolor|\n| 18|         5.8|        2.7|         4.1|        1.0|Iris-versicolor|\n| 19|         6.2|        2.2|         4.5|        1.5|Iris-versicolor|\n| 20|         5.6|        2.5|         3.9|        1.1|Iris-versicolor|\n+---+------------+-----------+------------+-----------+---------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nCONVERT TO DELTA parquet.`s3://deltasparktesting/iris_data_delta_10/`  PARTITIONED BY (CLASS_NAME String);",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Vacuum Command",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "Delta Lake doesn't physically remove files from storage for operations that logically delete the files (not contributing to the latest version of Delta Lake). You need to use the vacuum command to physically remove files from storage that have been marked for deletion and are older than the retention period.",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "The main benefit of vacuuming is to save on storage costs. Vacuuming does not make your queries run any faster and can limit your ability to time travel to earlier Delta table versions. You need to weigh the costs/benefits for each of your tables to develop an optimal vacuum strategy.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nfrom pyspark.sql.functions import *\nimport time\n\nut = time.time()\n\nproduct = [\n    {'product_id': '00001', 'product_name': 'Heater', 'price': 250, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00002', 'product_name': 'Thermostat', 'price': 400, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00003', 'product_name': 'Television', 'price': 600, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00004', 'product_name': 'Blender', 'price': 100, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00005', 'product_name': 'USB charger', 'price': 50, 'category': 'Electronics', 'updated_at': ut}\n]\n\ndf_products = spark.createDataFrame(Row(**x) for x in product)\ndf_products.show()\ndf_products.write.format(\"delta\").mode(\"overwrite\").option(\"path\",\"s3://deltasparktesting/iris_data_delta_6/\").saveAsTable(f\"deltalake_db.delta_table_6\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: 44b7bfce-3ece-4143-af7f-6a70069eb207\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session 44b7bfce-3ece-4143-af7f-6a70069eb207 to get into ready status...\nSession 44b7bfce-3ece-4143-af7f-6a70069eb207 has been created.\n+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00001|      Heater|  250|Electronics|1.7393566207408683E9|\n|     00002|  Thermostat|  400|Electronics|1.7393566207408683E9|\n|     00003|  Television|  600|Electronics|1.7393566207408683E9|\n|     00004|     Blender|  100|Electronics|1.7393566207408683E9|\n|     00005| USB charger|   50|Electronics|1.7393566207408683E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df=spark.read.format('delta').load('s3://deltasparktesting/iris_data_delta_6/')\ndf.filter(df['product_id']=='00005').select(input_file_name()).show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------------------------------------------------------------------------------------------------------+\n|input_file_name()                                                                                           |\n+------------------------------------------------------------------------------------------------------------+\n|s3://deltasparktesting/iris_data_delta_6/part-00015-443ddce7-3d14-43d2-bc19-d84d39287d15-c000.snappy.parquet|\n+------------------------------------------------------------------------------------------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\ndelete from deltalake_db.delta_table_6 where product_id ='00005'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                1|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"VACUUM deltalake_db.delta_table_6 DRY RUN\").show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----+\n|path|\n+----+\n+----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "The vacuum command won't actually delete any files because the tombstoned files aren't older than the retention period, which is 7 days by default",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"VACUUM deltalake_db.delta_table_6 RETAIN 0 HOURS DRY RUN\").show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       \n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.conf.get(\"spark.databricks.delta.retentionDurationCheck.enabled\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "'true'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"VACUUM deltalake_db.delta_table_6 RETAIN 0 HOURS\").show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------------------------------+\n|path                                    |\n+----------------------------------------+\n|s3://deltasparktesting/iris_data_delta_6|\n+----------------------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"\"\"DESCRIBE HISTORY deltalake_db.delta_table_6\"\"\").show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+-------------------+------+--------+---------------------------------+----------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId|userName|operation                        |operationParameters                                                               |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                |userMetadata|engineInfo                                |\n+-------+-------------------+------+--------+---------------------------------+----------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|1      |2025-02-12 10:40:15|null  |null    |DELETE                           |{predicate -> [\"(spark_catalog.deltalake_db.delta_table_6.product_id = '00005')\"]}|null|null    |null     |0          |Serializable  |false        |{numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 2409, numDeletedRows -> 1, scanTimeMs -> 1653, numAddedFiles -> 1, rewriteTimeMs -> 755}|null        |Apache-Spark/3.3.0-amzn-1 Delta-Lake/2.1.0|\n|0      |2025-02-12 10:37:44|null  |null    |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> false, description -> null, partitionBy -> [], properties -> {}}    |null|null    |null     |null       |Serializable  |false        |{numFiles -> 6, numOutputRows -> 5, numOutputBytes -> 8514}                                                                                                                     |null        |Apache-Spark/3.3.0-amzn-1 Delta-Lake/2.1.0|\n+-------+-------------------+------+--------+---------------------------------+----------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_previous = spark.read.format(\"delta\").option(\"versionAsof\", 0).load('s3://deltasparktesting/iris_data_delta_6/')\ndf_previous.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JJavaError: An error occurred while calling o129.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 119.0 failed 4 times, most recent failure: Lost task 0.3 in stage 119.0 (TID 50775) (172.35.186.38 executor 2): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://deltasparktesting/iris_data_delta_6/part-00015-443ddce7-3d14-43d2-bc19-d84d39287d15-c000.snappy.parquet, range: 0-1589, partition values: [empty row], isDataPresent: false, eTag: null\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:702)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:383)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: No such file or directory 's3://deltasparktesting/iris_data_delta_6/part-00015-443ddce7-3d14-43d2-bc19-d84d39287d15-c000.snappy.parquet'\n\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:524)\n\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:538)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2250)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:533)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:486)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://deltasparktesting/iris_data_delta_6/part-00015-443ddce7-3d14-43d2-bc19-d84d39287d15-c000.snappy.parquet, range: 0-1589, partition values: [empty row], isDataPresent: false, eTag: null\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:702)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:383)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: No such file or directory 's3://deltasparktesting/iris_data_delta_6/part-00015-443ddce7-3d14-43d2-bc19-d84d39287d15-c000.snappy.parquet'\n\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:524)\n\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:538)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# V.V.I. Conclusion : If you run VACUUM on a Delta table, you lose the ability to time travel back to a version older than the specified data retention period.",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "# Remember to never remove Delta Lake table data files outside the context of the Delta Lake operations, as doing so can corrupt your tables",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "vacuum command will not run itself. To keep your prod Delta Tables tidy, you can setup a corn job to call vacuum command",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "delta.deletedFileRetentionDuration = \"interval \": controls how long ago a file must have been deleted before being a candidate for VACUUM",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# create table in metastore\nquery = f\"\"\"\nCREATE TABLE IF NOT EXISTS deltalake_db.delta_table_11 (\n  ID INT,\n  SEPAL_LENGTH double,\n  SEPAL_WIDTH double,\n  PETAL_LENGTH double,\n  PETAL_WIDTH double,\n  CLASS_NAME STRING\n)\nUSING delta\nLOCATION 's3://deltasparktesting/delta_table_11/'\nTBLPROPERTIES (\n    'delta.deletedFileRetentionDuration' = 'interval 0 days'\n);\n\"\"\"\n\nspark.sql(query)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: native-delta-sql--ce361057-65cf-4290-ac87-695e5ee0ea40\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session native-delta-sql--ce361057-65cf-4290-ac87-695e5ee0ea40 to get into ready status...\nSession native-delta-sql--ce361057-65cf-4290-ac87-695e5ee0ea40 has been created.\nDataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\ninsert into deltalake_db.delta_table_11\nselect * from deltalake_db.csv_table;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql # Read table from metastore\nDelete FROM deltalake_db.delta_table_11 where CLASS_NAME='Iris-versicolor';",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|               51|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nVACUUM deltalake_db.delta_table_11 DRY RUN",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+\n|                path|\n+--------------------+\n|s3://deltasparkte...|\n+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nVACUUM deltalake_db.delta_table_11",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+\n|                path|\n+--------------------+\n|s3://deltasparkte...|\n+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Scaling Massive Metadata",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "Now that we have seen how transaction log records each operation, we can have many very large files with thousands of transaction log entries for a single DeltaLake Table. \n\nHow DeltaLake scale it's metadata handling wihtout needing to read thousands of small files, which would negatively impact Spark's reading performace?",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "To alleviate this issue, Delta Lake creates a checkpoint file in Parquet format after it creates (by default) the 10th commit (i.e. transaction). ",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "These checkpoint files save the entire state of the table at a point in time – in native Parquet format that is quick and easy for any engine to read. It offers the reader a “shortcut” to fully reproducing a table’s state to avoid reprocessing what could be thousands of tiny, inefficient JSON files.",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "https://www.databricks.com/wp-content/uploads/2019/08/image6-1.png",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "https://www.databricks.com/wp-content/uploads/2019/08/image2-3.png",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Row\nfrom pyspark.sql.functions import *\nimport time\n\nut = time.time()\n\nproduct = [\n    {'product_id': '00001', 'product_name': 'Heater', 'price': 250, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00002', 'product_name': 'Thermostat', 'price': 400, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00003', 'product_name': 'Television', 'price': 600, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00004', 'product_name': 'Blender', 'price': 100, 'category': 'Electronics', 'updated_at': ut},\n    {'product_id': '00005', 'product_name': 'USB charger', 'price': 50, 'category': 'Electronics', 'updated_at': ut}\n]\n\ndf_products = spark.createDataFrame(Row(**x) for x in product)\ndf_products.show()\ndf_products.write.format(\"delta\").mode(\"overwrite\").option(\"path\",\"s3://deltasparktesting/iris_data_delta_13/\").saveAsTable(f\"deltalake_db.delta_table_13\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+-----+-----------+--------------------+\n|product_id|product_name|price|   category|          updated_at|\n+----------+------------+-----+-----------+--------------------+\n|     00001|      Heater|  250|Electronics|1.7402074193605444E9|\n|     00002|  Thermostat|  400|Electronics|1.7402074193605444E9|\n|     00003|  Television|  600|Electronics|1.7402074193605444E9|\n|     00004|     Blender|  100|Electronics|1.7402074193605444E9|\n|     00005| USB charger|   50|Electronics|1.7402074193605444E9|\n+----------+------------+-----+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nUPDATE deltalake_db.delta_table_13 \nSET product_name = 'Cooker' \nWHERE product_id = '00004';",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                1|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nUPDATE deltalake_db.delta_table_13 \nSET price = price * 1.1;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                5|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nUPDATE deltalake_db.delta_table_13 \nSET category = 'Accessories' \nWHERE product_id = '00005';",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                1|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nINSERT INTO deltalake_db.delta_table_13 \nVALUES ('00006', 'Smartphone', 800, 'Electronics', current_timestamp());",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nINSERT INTO deltalake_db.delta_table_13 \nVALUES ('00007', 'Laptop', 1200, 'Electronics', current_timestamp());",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nDELETE FROM deltalake_db.delta_table_13 \nWHERE product_id = '00004';",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                1|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nUPDATE deltalake_db.delta_table_13 \nSET price = price * 0.95 \nWHERE category = 'Electronics';",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                5|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nALTER TABLE deltalake_db.delta_table_13 \nADD COLUMNS (discount DOUBLE);",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nUPDATE deltalake_db.delta_table_13 SET discount = 0.10;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                6|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nUPDATE deltalake_db.delta_table_13 \nSET price = price * (1 - discount);",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                6|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "checkpoint_df=spark.read.format('parquet').load('s3://deltasparktesting/iris_data_delta_13/_delta_log/00000000000000000010.checkpoint.parquet')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "checkpoint_df.show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+--------+--------+\n|txn |add                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |remove                                                                                                           |metaData|protocol|\n+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+--------+--------+\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00000-0809251d-2c52-4a40-9e21-4e4160d73a7a-c000.snappy.parquet, 1740207624334, false, true, {}, 1850, null}|null    |null    |\n|null|{part-00003-12befe53-628b-4d9c-baf8-1838ee73589a-c000.snappy.parquet, {}, 1842, 1740207625000, false, null, {\"numRecords\":1,\"minValues\":{\"product_id\":\"00003\",\"product_name\":\"Television\",\"price\":564,\"category\":\"Electronics\",\"updated_at\":1.7402074193605444E9,\"discount\":0.1},\"maxValues\":{\"product_id\":\"00003\",\"product_name\":\"Television\",\"price\":564,\"category\":\"Electronics\",\"updated_at\":1.7402074193605444E9,\"discount\":0.1},\"nullCount\":{\"product_id\":0,\"product_name\":0,\"price\":0,\"category\":0,\"updated_at\":0,\"discount\":0}}}|null                                                                                                             |null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00000-147b3818-8bbc-4d7c-b3d7-3f3e6373b29a-c000.snappy.parquet, 1740207606637, false, true, {}, 1588, null}|null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00004-c6b5fa73-296e-4ce3-8ce2-9fd6be60f103-c000.snappy.parquet, 1740207624334, false, true, {}, 1816, null}|null    |null    |\n|null|{part-00004-f85a75ce-ca3c-4b63-b5f8-21bf56350ecc-c000.snappy.parquet, {}, 1816, 1740207625000, false, null, {\"numRecords\":1,\"minValues\":{\"product_id\":\"00007\",\"product_name\":\"Laptop\",\"price\":1026,\"category\":\"Electronics\",\"updated_at\":1.740207564753E9,\"discount\":0.1},\"maxValues\":{\"product_id\":\"00007\",\"product_name\":\"Laptop\",\"price\":1026,\"category\":\"Electronics\",\"updated_at\":1.740207564753E9,\"discount\":0.1},\"nullCount\":{\"product_id\":0,\"product_name\":0,\"price\":0,\"category\":0,\"updated_at\":0,\"discount\":0}}}              |null                                                                                                             |null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00000-bec4822d-34e6-4c94-8b8c-5697609168dc-c000.snappy.parquet, 1740207523498, false, true, {}, 1588, null}|null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00001-8646182c-0bf3-4457-81bc-fe8b5be29299-c000.snappy.parquet, 1740207585684, false, true, {}, 1581, null}|null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00001-524a9ee0-fd7d-457e-b556-fe4c40d36b34-c000.snappy.parquet, 1740207624334, false, true, {}, 1844, null}|null    |null    |\n|null|{part-00002-63fd6edb-4aa9-4835-8f15-fee30aae1fb7-c000.snappy.parquet, {}, 1843, 1740207625000, false, null, {\"numRecords\":1,\"minValues\":{\"product_id\":\"00002\",\"product_name\":\"Thermostat\",\"price\":376,\"category\":\"Electronics\",\"updated_at\":1.7402074193605444E9,\"discount\":0.1},\"maxValues\":{\"product_id\":\"00002\",\"product_name\":\"Thermostat\",\"price\":376,\"category\":\"Electronics\",\"updated_at\":1.7402074193605444E9,\"discount\":0.1},\"nullCount\":{\"product_id\":0,\"product_name\":0,\"price\":0,\"category\":0,\"updated_at\":0,\"discount\":0}}}|null                                                                                                             |null    |null    |\n|null|{part-00000-a0c709d9-e8a5-4606-ae50-5ba59da35a63-c000.snappy.parquet, {}, 1850, 1740207625000, false, null, {\"numRecords\":1,\"minValues\":{\"product_id\":\"00005\",\"product_name\":\"USB charger\",\"price\":49,\"category\":\"Accessories\",\"updated_at\":1.7402074193605444E9,\"discount\":0.1},\"maxValues\":{\"product_id\":\"00005\",\"product_name\":\"USB charger\",\"price\":49,\"category\":\"Accessories\",\"updated_at\":1.7402074193605444E9,\"discount\":0.1},\"nullCount\":{\"product_id\":0,\"product_name\":0,\"price\":0,\"category\":0,\"updated_at\":0,\"discount\":0}}}|null                                                                                                             |null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00000-ee01cfda-1c44-4e4a-a655-d1303f69111c-c000.snappy.parquet, 1740207585684, false, true, {}, 1582, null}|null    |null    |\n|null|{part-00005-c7866c1b-c1b0-4d86-8602-63018cec7bcc-c000.snappy.parquet, {}, 1815, 1740207625000, false, null, {\"numRecords\":1,\"minValues\":{\"product_id\":\"00001\",\"product_name\":\"Heater\",\"price\":234,\"category\":\"Electronics\",\"updated_at\":1.7402074193605444E9,\"discount\":0.1},\"maxValues\":{\"product_id\":\"00001\",\"product_name\":\"Heater\",\"price\":234,\"category\":\"Electronics\",\"updated_at\":1.7402074193605444E9,\"discount\":0.1},\"nullCount\":{\"product_id\":0,\"product_name\":0,\"price\":0,\"category\":0,\"updated_at\":0,\"discount\":0}}}        |null                                                                                                             |null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00003-522aa262-ae6a-4478-b1a5-0449aec3b536-c000.snappy.parquet, 1740207606637, false, true, {}, 1554, null}|null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00002-05038935-669a-4a73-8663-cc88e87d790b-c000.snappy.parquet, 1740207585684, false, true, {}, 1581, null}|null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00009-2193c5d6-ae4e-4213-bf54-5e492a1df38b-c000.snappy.parquet, 1740207519673, false, true, {}, 1581, null}|null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00000-b8d5cee5-b476-4755-92ea-d80c89ab6629-c000.snappy.parquet, 1740207606637, false, true, {}, 1582, null}|null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00015-b68ed2c5-7bb5-4ece-b5bd-b5ca43d06697-c000.snappy.parquet, 1740207519673, false, true, {}, 1588, null}|null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00004-bdb418eb-0f4b-4253-814a-f0002d801994-c000.snappy.parquet, 1740207585684, false, true, {}, 1553, null}|null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00005-4f3d0a8a-7366-4c35-8916-9f65de2eddac-c000.snappy.parquet, 1740207624334, false, true, {}, 1815, null}|null    |null    |\n|null|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |{part-00000-fc6a7de1-4b24-4bd3-95f5-509f94b2e37f-c000.snappy.parquet, 1740207519673, false, true, {}, 647, null} |null    |null    |\n+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+--------+--------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Controlling the checkpoint interval",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"\"\"Create Table deltalake_db.delta_table_14 (\n  product_id String,\n  product_name STRING,\n  price Int,\n  category STRING,\n  updated_at Int\n)  using delta LOCATION 's3://deltasparktesting/iris_data_delta_14/' TBLPROPERTIES ('delta.checkpointInterval'=5)\"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nINSERT INTO deltalake_db.delta_table_14 (product_id, product_name, price, category, updated_at)  VALUES ('00001', 'Heater', 250, 'Electronics', 1700000000);",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nINSERT INTO deltalake_db.delta_table_14 (product_id, product_name, price, category, updated_at) \nVALUES ('00002', 'Thermostat', 400, 'Electronics', 1700000000);",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\n\nINSERT INTO deltalake_db.delta_table_14 (product_id, product_name, price, category, updated_at) \nVALUES ('00003', 'Television', 600, 'Electronics', 1700000000);",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\n\nINSERT INTO deltalake_db.delta_table_14 (product_id, product_name, price, category, updated_at) \nVALUES ('00004', 'Blender', 100, 'Electronics', 1700000000);",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\n\nINSERT INTO deltalake_db.delta_table_14 (product_id, product_name, price, category, updated_at) \nVALUES ('00005', 'USB charger', 50, 'Electronics', 1700000000);",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nINSERT INTO deltalake_db.delta_table_14 (product_id, product_name, price, category, updated_at) \nVALUES ('00005', 'USB charger', 50, 'Electronics', 1700000000);",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# Change Data Feed",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%%sql\nCREATE TABLE sales_data_demo_yt (\n    id INT,\n    product STRING,\n    quantity INT,\n    price DOUBLE\n) USING DELTA\nTBLPROPERTIES (delta.enableChangeDataFeed = true)\nLocation 's3://deltasparktesting/incremental_test_yt/';",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: 47e2e12b-de33-4a54-ae34-e96cda9829fb\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session 47e2e12b-de33-4a54-ae34-e96cda9829fb to get into ready status...\nSession 47e2e12b-de33-4a54-ae34-e96cda9829fb has been created.\n++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nselect * from sales_data_demo_yt;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+--------+-----+\n| id|product|quantity|price|\n+---+-------+--------+-----+\n+---+-------+--------+-----+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nINSERT INTO sales_data_demo_yt VALUES \n(1, 'Laptop', 5, 1200.00),\n(2, 'Phone', 10, 800.00);",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"1970-01-01 00:00:00\") \\\n    .load('s3://deltasparktesting/incremental_test_yt/')\ndf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+--------+------+------------+---------------+-------------------+\n| id|product|quantity| price|_change_type|_commit_version|  _commit_timestamp|\n+---+-------+--------+------+------------+---------------+-------------------+\n|  1| Laptop|       5|1200.0|      insert|              1|2025-02-27 04:32:18|\n|  2|  Phone|      10| 800.0|      insert|              1|2025-02-27 04:32:18|\n+---+-------+--------+------+------------+---------------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nselect * from sales_data_demo_yt;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+--------+------+\n| id|product|quantity| price|\n+---+-------+--------+------+\n|  1| Laptop|       5|1200.0|\n|  2|  Phone|      10| 800.0|\n+---+-------+--------+------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select current_timestamp()\").show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------------+\n|current_timestamp()    |\n+-----------------------+\n|2025-02-27 04:34:52.505|\n+-----------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nUPDATE sales_data_demo_yt SET price = 1100.00 WHERE id = 1;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                1|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nselect * from sales_data_demo_yt;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+--------+------+\n| id|product|quantity| price|\n+---+-------+--------+------+\n|  1| Laptop|       5|1100.0|\n|  2|  Phone|      10| 800.0|\n+---+-------+--------+------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2025-02-27 04:34:52.505\") \\\n    .load('s3://deltasparktesting/incremental_test_yt/')\ndf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+--------+------+----------------+---------------+-------------------+\n| id|product|quantity| price|    _change_type|_commit_version|  _commit_timestamp|\n+---+-------+--------+------+----------------+---------------+-------------------+\n|  1| Laptop|       5|1200.0| update_preimage|              2|2025-02-27 04:35:23|\n|  1| Laptop|       5|1100.0|update_postimage|              2|2025-02-27 04:35:23|\n+---+-------+--------+------+----------------+---------------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"select current_timestamp()\").show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------------+\n|current_timestamp()    |\n+-----------------------+\n|2025-02-27 04:38:29.683|\n+-----------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nDELETE FROM sales_data_demo_yt WHERE id = 2;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                1|\n+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%sql\nselect * from sales_data_demo_yt;",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+--------+------+\n| id|product|quantity| price|\n+---+-------+--------+------+\n|  1| Laptop|       5|1100.0|\n+---+-------+--------+------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2025-02-27 04:38:29.683\") \\\n    .load('s3://deltasparktesting/incremental_test_yt/')\ndf.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+--------+-----+------------+---------------+-------------------+\n| id|product|quantity|price|_change_type|_commit_version|  _commit_timestamp|\n+---+-------+--------+-----+------------+---------------+-------------------+\n|  2|  Phone|      10|800.0|      delete|              3|2025-02-27 04:38:39|\n+---+-------+--------+-----+------------+---------------+-------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df=spark.read.format('parquet').load('s3://deltasparktesting/incremental_test_yt/_change_data/');\ndf.show(20,truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+--------+------+----------------+\n|id |product|quantity|price |_change_type    |\n+---+-------+--------+------+----------------+\n|1  |Laptop |5       |1200.0|update_preimage |\n|1  |Laptop |5       |1100.0|update_postimage|\n|2  |Phone  |10      |800.0 |delete          |\n+---+-------+--------+------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}